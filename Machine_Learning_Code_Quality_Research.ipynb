{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning Code Quality Research",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KieranJP/Machine-Learning-Code-Quality-Research/blob/master/Machine_Learning_Code_Quality_Research.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLG3FxZkza1E",
        "colab_type": "text"
      },
      "source": [
        "# **Enter Training/Test Data**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BeaXaHJSxSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Uploading Test Data\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H927ZFIhi6Sx",
        "colab_type": "text"
      },
      "source": [
        "# **Nerual Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2nTbLCKS6_g",
        "colab_type": "code",
        "outputId": "7199d568-8e64-4f1c-93ac-8f00e0416191",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34896
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras as K\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot\n",
        "\n",
        "#Imports Required\n",
        "import numpy\n",
        "import pandas\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "#Selecting the Csv to use then Displaying it\n",
        "import pandas as pd\n",
        "dataset = pd.read_csv(\"FullELFFDataset.csv\")\n",
        "dataset.columns\n",
        "\n",
        "#Using sklearn's split function to split the data into training and testing data.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = dataset.values\n",
        "Y = dataset.Defective.values\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.3)\n",
        "\n",
        "X_gridtrain, X_gridtest, Y_gridtrain, Y_gridtest = train_test_split(X_train,Y_train, test_size=0.4)\n",
        "\n",
        "#Creates a NN\n",
        "model = Sequential()\n",
        "#Input Layer\n",
        "model.add(K.layers.Dense(units=40, input_dim=40, activation='relu', kernel_initializer='truncated_normal')) \n",
        "#Hidden Layers\n",
        "model.add(K.layers.Dense(units=32, activation='relu', kernel_initializer='truncated_normal')) \n",
        "model.add(K.layers.Dense(units=32, activation='relu', kernel_initializer='truncated_normal'))\n",
        "#Output Layers\n",
        "model.add(K.layers.Dense(units=1, activation='sigmoid', kernel_initializer='truncated_normal'))\n",
        "\n",
        "simple_sgd = K.optimizers.Adam(lr=0.00001)  \n",
        "model.compile(loss='binary_crossentropy', optimizer=simple_sgd, metrics=['accuracy']) \n",
        "\n",
        "#Find out the size of each class so can change weighting\n",
        "trueWeight = len(Y_train[Y_train==False])\n",
        "falseWeight = len(Y_train[Y_train==True])\n",
        "\n",
        "history = model.fit(X_train, Y_train, batch_size=128, epochs=1000, validation_split=0.2)\n",
        "\n",
        "Y_predict = model.predict(X_test)\n",
        "Y_predict =(Y_predict>0.5)\n",
        "print(confusion_matrix(Y_test, Y_predict))\n",
        "print(classification_report(Y_test, Y_predict))\n",
        "\n",
        "pyplot.plot(history.history['acc'])\n",
        "pyplot.plot(history.history['val_acc'])\n",
        "pyplot.title('Model Train vs Test Accuracy')\n",
        "pyplot.ylabel('accuracy')\n",
        "pyplot.xlabel('epoch')\n",
        "pyplot.legend(['train', 'test'], loc='upper left')\n",
        "pyplot.show()\n",
        "\n",
        "pyplot.plot(history.history['loss'])\n",
        "pyplot.plot(history.history['val_loss'])\n",
        "pyplot.title('Model Train vs Test loss')\n",
        "pyplot.ylabel('loss')\n",
        "pyplot.xlabel('epoch')\n",
        "pyplot.legend(['train', 'test'], loc='upper left')\n",
        "pyplot.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 54924 samples, validate on 13732 samples\n",
            "Epoch 1/1000\n",
            "54924/54924 [==============================] - 3s 59us/step - loss: 0.6161 - acc: 0.9480 - val_loss: 0.6004 - val_acc: 0.9493\n",
            "Epoch 2/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.5418 - acc: 0.9477 - val_loss: 0.5237 - val_acc: 0.9490\n",
            "Epoch 3/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.5126 - acc: 0.9471 - val_loss: 0.4854 - val_acc: 0.9493\n",
            "Epoch 4/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.4472 - acc: 0.9488 - val_loss: 0.4175 - val_acc: 0.9492\n",
            "Epoch 5/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.3960 - acc: 0.9487 - val_loss: 0.3761 - val_acc: 0.9454\n",
            "Epoch 6/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.3419 - acc: 0.9484 - val_loss: 0.3202 - val_acc: 0.9493\n",
            "Epoch 7/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.2907 - acc: 0.9487 - val_loss: 0.3062 - val_acc: 0.9493\n",
            "Epoch 8/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.2612 - acc: 0.9492 - val_loss: 0.2646 - val_acc: 0.9493\n",
            "Epoch 9/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.2505 - acc: 0.9476 - val_loss: 0.2350 - val_acc: 0.9488\n",
            "Epoch 10/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.2351 - acc: 0.9491 - val_loss: 0.2261 - val_acc: 0.9490\n",
            "Epoch 11/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.2262 - acc: 0.9480 - val_loss: 0.2319 - val_acc: 0.9493\n",
            "Epoch 12/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.2238 - acc: 0.9493 - val_loss: 0.3186 - val_acc: 0.9493\n",
            "Epoch 13/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.2216 - acc: 0.9488 - val_loss: 0.2148 - val_acc: 0.9492\n",
            "Epoch 14/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.2221 - acc: 0.9486 - val_loss: 0.2149 - val_acc: 0.9492\n",
            "Epoch 15/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.2150 - acc: 0.9492 - val_loss: 0.2126 - val_acc: 0.9439\n",
            "Epoch 16/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.2108 - acc: 0.9489 - val_loss: 0.2074 - val_acc: 0.9492\n",
            "Epoch 17/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.2101 - acc: 0.9485 - val_loss: 0.1994 - val_acc: 0.9492\n",
            "Epoch 18/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.2012 - acc: 0.9494 - val_loss: 0.1974 - val_acc: 0.9492\n",
            "Epoch 19/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.2034 - acc: 0.9487 - val_loss: 0.2008 - val_acc: 0.9491\n",
            "Epoch 20/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.2030 - acc: 0.9486 - val_loss: 0.2010 - val_acc: 0.9491\n",
            "Epoch 21/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1979 - acc: 0.9493 - val_loss: 0.1925 - val_acc: 0.9492\n",
            "Epoch 22/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1977 - acc: 0.9490 - val_loss: 0.2035 - val_acc: 0.9429\n",
            "Epoch 23/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1991 - acc: 0.9483 - val_loss: 0.1991 - val_acc: 0.9492\n",
            "Epoch 24/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1955 - acc: 0.9494 - val_loss: 0.2481 - val_acc: 0.9493\n",
            "Epoch 25/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1986 - acc: 0.9490 - val_loss: 0.1867 - val_acc: 0.9491\n",
            "Epoch 26/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1913 - acc: 0.9495 - val_loss: 0.1896 - val_acc: 0.9493\n",
            "Epoch 27/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1952 - acc: 0.9489 - val_loss: 0.1843 - val_acc: 0.9485\n",
            "Epoch 28/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1969 - acc: 0.9487 - val_loss: 0.1911 - val_acc: 0.9491\n",
            "Epoch 29/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1911 - acc: 0.9492 - val_loss: 0.1902 - val_acc: 0.9455\n",
            "Epoch 30/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1932 - acc: 0.9490 - val_loss: 0.2033 - val_acc: 0.9496\n",
            "Epoch 31/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.3223 - acc: 0.9499 - val_loss: 0.3552 - val_acc: 0.9494\n",
            "Epoch 32/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.2478 - acc: 0.9492 - val_loss: 0.1997 - val_acc: 0.9479\n",
            "Epoch 33/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.1959 - acc: 0.9485 - val_loss: 0.1932 - val_acc: 0.9485\n",
            "Epoch 34/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1920 - acc: 0.9489 - val_loss: 0.1866 - val_acc: 0.9490\n",
            "Epoch 35/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1907 - acc: 0.9492 - val_loss: 0.1932 - val_acc: 0.9495\n",
            "Epoch 36/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1887 - acc: 0.9489 - val_loss: 0.1821 - val_acc: 0.9473\n",
            "Epoch 37/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1902 - acc: 0.9489 - val_loss: 0.1886 - val_acc: 0.9492\n",
            "Epoch 38/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1885 - acc: 0.9489 - val_loss: 0.2060 - val_acc: 0.9410\n",
            "Epoch 39/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1954 - acc: 0.9487 - val_loss: 0.2227 - val_acc: 0.9493\n",
            "Epoch 40/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.1905 - acc: 0.9490 - val_loss: 0.1841 - val_acc: 0.9460\n",
            "Epoch 41/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1911 - acc: 0.9489 - val_loss: 0.1814 - val_acc: 0.9493\n",
            "Epoch 42/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1906 - acc: 0.9484 - val_loss: 0.1937 - val_acc: 0.9492\n",
            "Epoch 43/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1880 - acc: 0.9487 - val_loss: 0.1836 - val_acc: 0.9492\n",
            "Epoch 44/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1887 - acc: 0.9490 - val_loss: 0.1805 - val_acc: 0.9482\n",
            "Epoch 45/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.1925 - acc: 0.9481 - val_loss: 0.1907 - val_acc: 0.9492\n",
            "Epoch 46/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1860 - acc: 0.9495 - val_loss: 0.1898 - val_acc: 0.9493\n",
            "Epoch 47/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1900 - acc: 0.9488 - val_loss: 0.1920 - val_acc: 0.9491\n",
            "Epoch 48/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.1892 - acc: 0.9490 - val_loss: 0.2228 - val_acc: 0.9494\n",
            "Epoch 49/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.1850 - acc: 0.9494 - val_loss: 0.2367 - val_acc: 0.9493\n",
            "Epoch 50/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1886 - acc: 0.9485 - val_loss: 0.1777 - val_acc: 0.9495\n",
            "Epoch 51/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.1862 - acc: 0.9483 - val_loss: 0.1995 - val_acc: 0.9423\n",
            "Epoch 52/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1848 - acc: 0.9490 - val_loss: 0.1850 - val_acc: 0.9492\n",
            "Epoch 53/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1848 - acc: 0.9490 - val_loss: 0.1789 - val_acc: 0.9471\n",
            "Epoch 54/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1834 - acc: 0.9493 - val_loss: 0.1962 - val_acc: 0.9493\n",
            "Epoch 55/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1896 - acc: 0.9490 - val_loss: 0.1820 - val_acc: 0.9473\n",
            "Epoch 56/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1800 - acc: 0.9498 - val_loss: 0.1857 - val_acc: 0.9439\n",
            "Epoch 57/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1861 - acc: 0.9481 - val_loss: 0.1794 - val_acc: 0.9474\n",
            "Epoch 58/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1820 - acc: 0.9492 - val_loss: 0.2025 - val_acc: 0.9491\n",
            "Epoch 59/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1884 - acc: 0.9489 - val_loss: 0.1800 - val_acc: 0.9490\n",
            "Epoch 60/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1916 - acc: 0.9492 - val_loss: 0.1831 - val_acc: 0.9439\n",
            "Epoch 61/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1850 - acc: 0.9487 - val_loss: 0.1828 - val_acc: 0.9489\n",
            "Epoch 62/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1839 - acc: 0.9493 - val_loss: 0.1828 - val_acc: 0.9490\n",
            "Epoch 63/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1835 - acc: 0.9494 - val_loss: 0.1746 - val_acc: 0.9483\n",
            "Epoch 64/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.1871 - acc: 0.9481 - val_loss: 0.1965 - val_acc: 0.9430\n",
            "Epoch 65/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1884 - acc: 0.9488 - val_loss: 0.1871 - val_acc: 0.9434\n",
            "Epoch 66/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1801 - acc: 0.9498 - val_loss: 0.1817 - val_acc: 0.9452\n",
            "Epoch 67/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1917 - acc: 0.9491 - val_loss: 0.1798 - val_acc: 0.9490\n",
            "Epoch 68/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1803 - acc: 0.9491 - val_loss: 0.1809 - val_acc: 0.9489\n",
            "Epoch 69/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1847 - acc: 0.9487 - val_loss: 0.2157 - val_acc: 0.9376\n",
            "Epoch 70/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1950 - acc: 0.9474 - val_loss: 0.1793 - val_acc: 0.9492\n",
            "Epoch 71/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1821 - acc: 0.9492 - val_loss: 0.1767 - val_acc: 0.9484\n",
            "Epoch 72/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1864 - acc: 0.9490 - val_loss: 0.1835 - val_acc: 0.9487\n",
            "Epoch 73/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1876 - acc: 0.9487 - val_loss: 0.1814 - val_acc: 0.9479\n",
            "Epoch 74/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1810 - acc: 0.9488 - val_loss: 0.1764 - val_acc: 0.9476\n",
            "Epoch 75/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.1799 - acc: 0.9497 - val_loss: 0.1939 - val_acc: 0.9490\n",
            "Epoch 76/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1835 - acc: 0.9492 - val_loss: 0.1841 - val_acc: 0.9489\n",
            "Epoch 77/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1781 - acc: 0.9495 - val_loss: 0.1742 - val_acc: 0.9488\n",
            "Epoch 78/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.2014 - acc: 0.9490 - val_loss: 0.2002 - val_acc: 0.9489\n",
            "Epoch 79/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.1834 - acc: 0.9493 - val_loss: 0.2123 - val_acc: 0.9492\n",
            "Epoch 80/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1828 - acc: 0.9487 - val_loss: 0.1790 - val_acc: 0.9489\n",
            "Epoch 81/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.1832 - acc: 0.9491 - val_loss: 0.1772 - val_acc: 0.9488\n",
            "Epoch 82/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1820 - acc: 0.9489 - val_loss: 0.1777 - val_acc: 0.9465\n",
            "Epoch 83/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.1826 - acc: 0.9483 - val_loss: 0.1806 - val_acc: 0.9490\n",
            "Epoch 84/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1779 - acc: 0.9503 - val_loss: 0.1915 - val_acc: 0.9421\n",
            "Epoch 85/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1807 - acc: 0.9491 - val_loss: 0.1742 - val_acc: 0.9487\n",
            "Epoch 86/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1842 - acc: 0.9491 - val_loss: 0.1860 - val_acc: 0.9490\n",
            "Epoch 87/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1785 - acc: 0.9496 - val_loss: 0.1799 - val_acc: 0.9492\n",
            "Epoch 88/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1861 - acc: 0.9483 - val_loss: 0.1735 - val_acc: 0.9482\n",
            "Epoch 89/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.1807 - acc: 0.9493 - val_loss: 0.1774 - val_acc: 0.9489\n",
            "Epoch 90/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1859 - acc: 0.9500 - val_loss: 0.1796 - val_acc: 0.9491\n",
            "Epoch 91/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1790 - acc: 0.9493 - val_loss: 0.1781 - val_acc: 0.9489\n",
            "Epoch 92/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.1813 - acc: 0.9491 - val_loss: 0.1823 - val_acc: 0.9444\n",
            "Epoch 93/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1843 - acc: 0.9487 - val_loss: 0.1783 - val_acc: 0.9454\n",
            "Epoch 94/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1836 - acc: 0.9485 - val_loss: 0.2209 - val_acc: 0.9492\n",
            "Epoch 95/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1783 - acc: 0.9499 - val_loss: 0.1701 - val_acc: 0.9492\n",
            "Epoch 96/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1812 - acc: 0.9486 - val_loss: 0.1891 - val_acc: 0.9433\n",
            "Epoch 97/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1820 - acc: 0.9490 - val_loss: 0.1731 - val_acc: 0.9493\n",
            "Epoch 98/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1873 - acc: 0.9487 - val_loss: 0.1871 - val_acc: 0.9488\n",
            "Epoch 99/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1840 - acc: 0.9482 - val_loss: 0.1874 - val_acc: 0.9490\n",
            "Epoch 100/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.1759 - acc: 0.9490 - val_loss: 0.1863 - val_acc: 0.9490\n",
            "Epoch 101/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1783 - acc: 0.9494 - val_loss: 0.1794 - val_acc: 0.9492\n",
            "Epoch 102/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1996 - acc: 0.9488 - val_loss: 0.1809 - val_acc: 0.9489\n",
            "Epoch 103/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1758 - acc: 0.9498 - val_loss: 0.1795 - val_acc: 0.9441\n",
            "Epoch 104/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1797 - acc: 0.9485 - val_loss: 0.1707 - val_acc: 0.9492\n",
            "Epoch 105/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.2088 - acc: 0.9491 - val_loss: 0.1791 - val_acc: 0.9492\n",
            "Epoch 106/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1747 - acc: 0.9496 - val_loss: 0.1804 - val_acc: 0.9491\n",
            "Epoch 107/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1799 - acc: 0.9490 - val_loss: 0.1849 - val_acc: 0.9439\n",
            "Epoch 108/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.1798 - acc: 0.9493 - val_loss: 0.1777 - val_acc: 0.9492\n",
            "Epoch 109/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.1761 - acc: 0.9490 - val_loss: 0.1737 - val_acc: 0.9492\n",
            "Epoch 110/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1758 - acc: 0.9491 - val_loss: 0.1667 - val_acc: 0.9497\n",
            "Epoch 111/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1808 - acc: 0.9487 - val_loss: 0.1806 - val_acc: 0.9490\n",
            "Epoch 112/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1772 - acc: 0.9493 - val_loss: 0.1679 - val_acc: 0.9494\n",
            "Epoch 113/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1771 - acc: 0.9499 - val_loss: 0.2332 - val_acc: 0.9492\n",
            "Epoch 114/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1776 - acc: 0.9491 - val_loss: 0.1760 - val_acc: 0.9492\n",
            "Epoch 115/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1789 - acc: 0.9488 - val_loss: 0.1805 - val_acc: 0.9444\n",
            "Epoch 116/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.1752 - acc: 0.9492 - val_loss: 0.1669 - val_acc: 0.9496\n",
            "Epoch 117/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.1811 - acc: 0.9491 - val_loss: 0.1689 - val_acc: 0.9497\n",
            "Epoch 118/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.1768 - acc: 0.9495 - val_loss: 0.1914 - val_acc: 0.9418\n",
            "Epoch 119/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1785 - acc: 0.9490 - val_loss: 0.1689 - val_acc: 0.9497\n",
            "Epoch 120/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.1782 - acc: 0.9491 - val_loss: 0.1715 - val_acc: 0.9492\n",
            "Epoch 121/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1706 - acc: 0.9493 - val_loss: 0.1731 - val_acc: 0.9493\n",
            "Epoch 122/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1729 - acc: 0.9493 - val_loss: 0.1690 - val_acc: 0.9496\n",
            "Epoch 123/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1880 - acc: 0.9488 - val_loss: 0.1805 - val_acc: 0.9490\n",
            "Epoch 124/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1714 - acc: 0.9498 - val_loss: 0.2601 - val_acc: 0.9493\n",
            "Epoch 125/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1869 - acc: 0.9496 - val_loss: 0.1919 - val_acc: 0.9490\n",
            "Epoch 126/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1725 - acc: 0.9494 - val_loss: 0.1672 - val_acc: 0.9500\n",
            "Epoch 127/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1741 - acc: 0.9498 - val_loss: 0.2633 - val_acc: 0.9493\n",
            "Epoch 128/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1765 - acc: 0.9495 - val_loss: 0.1702 - val_acc: 0.9498\n",
            "Epoch 129/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.2002 - acc: 0.9496 - val_loss: 0.1842 - val_acc: 0.9496\n",
            "Epoch 130/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.1743 - acc: 0.9497 - val_loss: 0.1750 - val_acc: 0.9492\n",
            "Epoch 131/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1708 - acc: 0.9495 - val_loss: 0.1878 - val_acc: 0.9420\n",
            "Epoch 132/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.1774 - acc: 0.9492 - val_loss: 0.1732 - val_acc: 0.9499\n",
            "Epoch 133/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1727 - acc: 0.9499 - val_loss: 0.1944 - val_acc: 0.9491\n",
            "Epoch 134/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1751 - acc: 0.9496 - val_loss: 0.1667 - val_acc: 0.9487\n",
            "Epoch 135/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1733 - acc: 0.9498 - val_loss: 0.1824 - val_acc: 0.9438\n",
            "Epoch 136/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1709 - acc: 0.9501 - val_loss: 0.1791 - val_acc: 0.9493\n",
            "Epoch 137/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1781 - acc: 0.9492 - val_loss: 0.1822 - val_acc: 0.9456\n",
            "Epoch 138/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1795 - acc: 0.9486 - val_loss: 0.1743 - val_acc: 0.9493\n",
            "Epoch 139/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1702 - acc: 0.9501 - val_loss: 0.1636 - val_acc: 0.9503\n",
            "Epoch 140/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1726 - acc: 0.9494 - val_loss: 0.1732 - val_acc: 0.9498\n",
            "Epoch 141/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1705 - acc: 0.9495 - val_loss: 0.1639 - val_acc: 0.9505\n",
            "Epoch 142/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1815 - acc: 0.9485 - val_loss: 0.1800 - val_acc: 0.9493\n",
            "Epoch 143/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1722 - acc: 0.9497 - val_loss: 0.1713 - val_acc: 0.9495\n",
            "Epoch 144/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1726 - acc: 0.9493 - val_loss: 0.1882 - val_acc: 0.9492\n",
            "Epoch 145/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1715 - acc: 0.9489 - val_loss: 0.1646 - val_acc: 0.9500\n",
            "Epoch 146/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1905 - acc: 0.9492 - val_loss: 0.1779 - val_acc: 0.9497\n",
            "Epoch 147/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1792 - acc: 0.9487 - val_loss: 0.2406 - val_acc: 0.9493\n",
            "Epoch 148/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.1760 - acc: 0.9497 - val_loss: 0.1654 - val_acc: 0.9500\n",
            "Epoch 149/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1706 - acc: 0.9496 - val_loss: 0.1636 - val_acc: 0.9504\n",
            "Epoch 150/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1684 - acc: 0.9499 - val_loss: 0.1707 - val_acc: 0.9496\n",
            "Epoch 151/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1792 - acc: 0.9498 - val_loss: 0.1675 - val_acc: 0.9501\n",
            "Epoch 152/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1739 - acc: 0.9495 - val_loss: 0.1669 - val_acc: 0.9487\n",
            "Epoch 153/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1730 - acc: 0.9497 - val_loss: 0.1647 - val_acc: 0.9506\n",
            "Epoch 154/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.1753 - acc: 0.9499 - val_loss: 0.1800 - val_acc: 0.9492\n",
            "Epoch 155/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1754 - acc: 0.9487 - val_loss: 0.1657 - val_acc: 0.9504\n",
            "Epoch 156/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1737 - acc: 0.9493 - val_loss: 0.1863 - val_acc: 0.9493\n",
            "Epoch 157/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1778 - acc: 0.9501 - val_loss: 0.1664 - val_acc: 0.9481\n",
            "Epoch 158/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1693 - acc: 0.9498 - val_loss: 0.1807 - val_acc: 0.9493\n",
            "Epoch 159/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1711 - acc: 0.9500 - val_loss: 0.1766 - val_acc: 0.9444\n",
            "Epoch 160/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1760 - acc: 0.9495 - val_loss: 0.1899 - val_acc: 0.9421\n",
            "Epoch 161/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1713 - acc: 0.9501 - val_loss: 0.1685 - val_acc: 0.9496\n",
            "Epoch 162/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1675 - acc: 0.9500 - val_loss: 0.1605 - val_acc: 0.9504\n",
            "Epoch 163/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1750 - acc: 0.9497 - val_loss: 0.2487 - val_acc: 0.9494\n",
            "Epoch 164/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1797 - acc: 0.9494 - val_loss: 0.1671 - val_acc: 0.9503\n",
            "Epoch 165/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1713 - acc: 0.9497 - val_loss: 0.1620 - val_acc: 0.9507\n",
            "Epoch 166/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1685 - acc: 0.9493 - val_loss: 0.1711 - val_acc: 0.9497\n",
            "Epoch 167/1000\n",
            "54924/54924 [==============================] - 2s 34us/step - loss: 0.1715 - acc: 0.9495 - val_loss: 0.1625 - val_acc: 0.9506\n",
            "Epoch 168/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1669 - acc: 0.9501 - val_loss: 0.1676 - val_acc: 0.9498\n",
            "Epoch 169/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1736 - acc: 0.9491 - val_loss: 0.1658 - val_acc: 0.9500\n",
            "Epoch 170/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1646 - acc: 0.9503 - val_loss: 0.1638 - val_acc: 0.9488\n",
            "Epoch 171/1000\n",
            "54924/54924 [==============================] - 2s 34us/step - loss: 0.1702 - acc: 0.9496 - val_loss: 0.1645 - val_acc: 0.9484\n",
            "Epoch 172/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1670 - acc: 0.9499 - val_loss: 0.1724 - val_acc: 0.9498\n",
            "Epoch 173/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1668 - acc: 0.9503 - val_loss: 0.1653 - val_acc: 0.9498\n",
            "Epoch 174/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.1688 - acc: 0.9501 - val_loss: 0.1890 - val_acc: 0.9409\n",
            "Epoch 175/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1687 - acc: 0.9500 - val_loss: 0.1611 - val_acc: 0.9503\n",
            "Epoch 176/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1679 - acc: 0.9500 - val_loss: 0.1724 - val_acc: 0.9498\n",
            "Epoch 177/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1679 - acc: 0.9503 - val_loss: 0.1593 - val_acc: 0.9502\n",
            "Epoch 178/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1669 - acc: 0.9499 - val_loss: 0.1587 - val_acc: 0.9501\n",
            "Epoch 179/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1679 - acc: 0.9499 - val_loss: 0.1614 - val_acc: 0.9506\n",
            "Epoch 180/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.1659 - acc: 0.9499 - val_loss: 0.1594 - val_acc: 0.9505\n",
            "Epoch 181/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1651 - acc: 0.9499 - val_loss: 0.1589 - val_acc: 0.9505\n",
            "Epoch 182/1000\n",
            "54924/54924 [==============================] - 2s 33us/step - loss: 0.1661 - acc: 0.9502 - val_loss: 0.1588 - val_acc: 0.9504\n",
            "Epoch 183/1000\n",
            "54924/54924 [==============================] - 2s 34us/step - loss: 0.1671 - acc: 0.9501 - val_loss: 0.1577 - val_acc: 0.9511\n",
            "Epoch 184/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1700 - acc: 0.9487 - val_loss: 0.1814 - val_acc: 0.9495\n",
            "Epoch 185/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1645 - acc: 0.9504 - val_loss: 0.1848 - val_acc: 0.9429\n",
            "Epoch 186/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1656 - acc: 0.9500 - val_loss: 0.1813 - val_acc: 0.9495\n",
            "Epoch 187/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1630 - acc: 0.9504 - val_loss: 0.1685 - val_acc: 0.9499\n",
            "Epoch 188/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1632 - acc: 0.9504 - val_loss: 0.1569 - val_acc: 0.9508\n",
            "Epoch 189/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1759 - acc: 0.9495 - val_loss: 0.1798 - val_acc: 0.9496\n",
            "Epoch 190/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1644 - acc: 0.9502 - val_loss: 0.1937 - val_acc: 0.9495\n",
            "Epoch 191/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1948 - acc: 0.9499 - val_loss: 0.1687 - val_acc: 0.9505\n",
            "Epoch 192/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.1665 - acc: 0.9501 - val_loss: 0.1872 - val_acc: 0.9494\n",
            "Epoch 193/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1672 - acc: 0.9496 - val_loss: 0.1583 - val_acc: 0.9503\n",
            "Epoch 194/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.2038 - acc: 0.9503 - val_loss: 0.1701 - val_acc: 0.9508\n",
            "Epoch 195/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.1659 - acc: 0.9508 - val_loss: 0.1614 - val_acc: 0.9487\n",
            "Epoch 196/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1652 - acc: 0.9506 - val_loss: 0.1593 - val_acc: 0.9493\n",
            "Epoch 197/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1627 - acc: 0.9503 - val_loss: 0.1855 - val_acc: 0.9495\n",
            "Epoch 198/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1645 - acc: 0.9507 - val_loss: 0.1628 - val_acc: 0.9503\n",
            "Epoch 199/1000\n",
            "54924/54924 [==============================] - 2s 30us/step - loss: 0.1669 - acc: 0.9499 - val_loss: 0.1702 - val_acc: 0.9497\n",
            "Epoch 200/1000\n",
            "54924/54924 [==============================] - 2s 29us/step - loss: 0.1635 - acc: 0.9505 - val_loss: 0.2015 - val_acc: 0.9378\n",
            "Epoch 201/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1688 - acc: 0.9494 - val_loss: 0.1787 - val_acc: 0.9496\n",
            "Epoch 202/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.1632 - acc: 0.9508 - val_loss: 0.1536 - val_acc: 0.9510\n",
            "Epoch 203/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1656 - acc: 0.9499 - val_loss: 0.1582 - val_acc: 0.9508\n",
            "Epoch 204/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1563 - acc: 0.9513 - val_loss: 0.2537 - val_acc: 0.9267\n",
            "Epoch 205/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1808 - acc: 0.9476 - val_loss: 0.1581 - val_acc: 0.9502\n",
            "Epoch 206/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1588 - acc: 0.9510 - val_loss: 0.1670 - val_acc: 0.9469\n",
            "Epoch 207/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.1642 - acc: 0.9504 - val_loss: 0.1623 - val_acc: 0.9502\n",
            "Epoch 208/1000\n",
            "54924/54924 [==============================] - 2s 32us/step - loss: 0.1647 - acc: 0.9502 - val_loss: 0.1561 - val_acc: 0.9513\n",
            "Epoch 209/1000\n",
            "54924/54924 [==============================] - 2s 31us/step - loss: 0.1631 - acc: 0.9507 - val_loss: 0.1627 - val_acc: 0.9501\n",
            "Epoch 210/1000\n",
            "54924/54924 [==============================] - 2s 29us/step - loss: 0.1677 - acc: 0.9505 - val_loss: 0.1607 - val_acc: 0.9508\n",
            "Epoch 211/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1621 - acc: 0.9507 - val_loss: 0.1631 - val_acc: 0.9502\n",
            "Epoch 212/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1650 - acc: 0.9504 - val_loss: 0.1532 - val_acc: 0.9518\n",
            "Epoch 213/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1589 - acc: 0.9510 - val_loss: 0.1723 - val_acc: 0.9451\n",
            "Epoch 214/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1585 - acc: 0.9514 - val_loss: 0.1552 - val_acc: 0.9512\n",
            "Epoch 215/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1623 - acc: 0.9510 - val_loss: 0.1595 - val_acc: 0.9511\n",
            "Epoch 216/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1606 - acc: 0.9506 - val_loss: 0.1567 - val_acc: 0.9508\n",
            "Epoch 217/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.2647 - acc: 0.9505 - val_loss: 0.4300 - val_acc: 0.9493\n",
            "Epoch 218/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.2774 - acc: 0.9490 - val_loss: 0.1911 - val_acc: 0.9498\n",
            "Epoch 219/1000\n",
            "54924/54924 [==============================] - 2s 33us/step - loss: 0.1685 - acc: 0.9509 - val_loss: 0.1774 - val_acc: 0.9445\n",
            "Epoch 220/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1606 - acc: 0.9512 - val_loss: 0.1529 - val_acc: 0.9510\n",
            "Epoch 221/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1562 - acc: 0.9512 - val_loss: 0.1600 - val_acc: 0.9509\n",
            "Epoch 222/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1604 - acc: 0.9505 - val_loss: 0.1759 - val_acc: 0.9498\n",
            "Epoch 223/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1597 - acc: 0.9507 - val_loss: 0.1679 - val_acc: 0.9500\n",
            "Epoch 224/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1585 - acc: 0.9508 - val_loss: 0.1646 - val_acc: 0.9501\n",
            "Epoch 225/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.1636 - acc: 0.9503 - val_loss: 0.1539 - val_acc: 0.9510\n",
            "Epoch 226/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.1621 - acc: 0.9504 - val_loss: 0.1519 - val_acc: 0.9517\n",
            "Epoch 227/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1627 - acc: 0.9507 - val_loss: 0.1935 - val_acc: 0.9412\n",
            "Epoch 228/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1562 - acc: 0.9515 - val_loss: 0.1520 - val_acc: 0.9517\n",
            "Epoch 229/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1578 - acc: 0.9511 - val_loss: 0.1694 - val_acc: 0.9469\n",
            "Epoch 230/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1630 - acc: 0.9508 - val_loss: 0.1525 - val_acc: 0.9515\n",
            "Epoch 231/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1621 - acc: 0.9503 - val_loss: 0.1562 - val_acc: 0.9506\n",
            "Epoch 232/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1687 - acc: 0.9491 - val_loss: 0.1699 - val_acc: 0.9500\n",
            "Epoch 233/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1597 - acc: 0.9509 - val_loss: 0.1512 - val_acc: 0.9518\n",
            "Epoch 234/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1566 - acc: 0.9510 - val_loss: 0.1520 - val_acc: 0.9505\n",
            "Epoch 235/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1597 - acc: 0.9506 - val_loss: 0.1507 - val_acc: 0.9518\n",
            "Epoch 236/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1634 - acc: 0.9501 - val_loss: 0.1664 - val_acc: 0.9503\n",
            "Epoch 237/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1562 - acc: 0.9512 - val_loss: 0.2112 - val_acc: 0.9495\n",
            "Epoch 238/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1669 - acc: 0.9503 - val_loss: 0.1473 - val_acc: 0.9516\n",
            "Epoch 239/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1576 - acc: 0.9510 - val_loss: 0.1613 - val_acc: 0.9505\n",
            "Epoch 240/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1639 - acc: 0.9497 - val_loss: 0.1649 - val_acc: 0.9502\n",
            "Epoch 241/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1530 - acc: 0.9515 - val_loss: 0.1578 - val_acc: 0.9507\n",
            "Epoch 242/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1522 - acc: 0.9520 - val_loss: 0.1768 - val_acc: 0.9501\n",
            "Epoch 243/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.1618 - acc: 0.9505 - val_loss: 0.1835 - val_acc: 0.9426\n",
            "Epoch 244/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1528 - acc: 0.9515 - val_loss: 0.1630 - val_acc: 0.9503\n",
            "Epoch 245/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1596 - acc: 0.9513 - val_loss: 0.1497 - val_acc: 0.9524\n",
            "Epoch 246/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1545 - acc: 0.9514 - val_loss: 0.1979 - val_acc: 0.9499\n",
            "Epoch 247/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1595 - acc: 0.9504 - val_loss: 0.1627 - val_acc: 0.9505\n",
            "Epoch 248/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.1535 - acc: 0.9519 - val_loss: 0.1709 - val_acc: 0.9506\n",
            "Epoch 249/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1631 - acc: 0.9500 - val_loss: 0.1593 - val_acc: 0.9511\n",
            "Epoch 250/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1556 - acc: 0.9509 - val_loss: 0.1448 - val_acc: 0.9527\n",
            "Epoch 251/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1523 - acc: 0.9514 - val_loss: 0.1507 - val_acc: 0.9524\n",
            "Epoch 252/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1537 - acc: 0.9521 - val_loss: 0.1554 - val_acc: 0.9516\n",
            "Epoch 253/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1552 - acc: 0.9513 - val_loss: 0.1683 - val_acc: 0.9507\n",
            "Epoch 254/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1619 - acc: 0.9508 - val_loss: 0.1848 - val_acc: 0.9421\n",
            "Epoch 255/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1591 - acc: 0.9512 - val_loss: 0.2134 - val_acc: 0.9498\n",
            "Epoch 256/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1590 - acc: 0.9509 - val_loss: 0.1901 - val_acc: 0.9501\n",
            "Epoch 257/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1504 - acc: 0.9514 - val_loss: 0.2106 - val_acc: 0.9498\n",
            "Epoch 258/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1594 - acc: 0.9511 - val_loss: 0.1806 - val_acc: 0.9505\n",
            "Epoch 259/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1519 - acc: 0.9518 - val_loss: 0.1504 - val_acc: 0.9522\n",
            "Epoch 260/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1493 - acc: 0.9523 - val_loss: 0.1424 - val_acc: 0.9529\n",
            "Epoch 261/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1608 - acc: 0.9508 - val_loss: 0.1798 - val_acc: 0.9503\n",
            "Epoch 262/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1520 - acc: 0.9517 - val_loss: 0.1424 - val_acc: 0.9530\n",
            "Epoch 263/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1492 - acc: 0.9516 - val_loss: 0.1481 - val_acc: 0.9524\n",
            "Epoch 264/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1509 - acc: 0.9516 - val_loss: 0.1480 - val_acc: 0.9520\n",
            "Epoch 265/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1623 - acc: 0.9507 - val_loss: 0.1435 - val_acc: 0.9525\n",
            "Epoch 266/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.1526 - acc: 0.9508 - val_loss: 0.1555 - val_acc: 0.9514\n",
            "Epoch 267/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1453 - acc: 0.9525 - val_loss: 0.1866 - val_acc: 0.9390\n",
            "Epoch 268/1000\n",
            "54924/54924 [==============================] - 2s 34us/step - loss: 0.1514 - acc: 0.9506 - val_loss: 0.1619 - val_acc: 0.9471\n",
            "Epoch 269/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1499 - acc: 0.9513 - val_loss: 0.1569 - val_acc: 0.9509\n",
            "Epoch 270/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1437 - acc: 0.9525 - val_loss: 0.1741 - val_acc: 0.9434\n",
            "Epoch 271/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1511 - acc: 0.9517 - val_loss: 0.1463 - val_acc: 0.9534\n",
            "Epoch 272/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1562 - acc: 0.9518 - val_loss: 0.1489 - val_acc: 0.9522\n",
            "Epoch 273/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1502 - acc: 0.9523 - val_loss: 0.1536 - val_acc: 0.9514\n",
            "Epoch 274/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1485 - acc: 0.9529 - val_loss: 0.1691 - val_acc: 0.9509\n",
            "Epoch 275/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1438 - acc: 0.9535 - val_loss: 0.1504 - val_acc: 0.9519\n",
            "Epoch 276/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1496 - acc: 0.9518 - val_loss: 0.1592 - val_acc: 0.9487\n",
            "Epoch 277/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1469 - acc: 0.9519 - val_loss: 0.1483 - val_acc: 0.9520\n",
            "Epoch 278/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1462 - acc: 0.9523 - val_loss: 0.1421 - val_acc: 0.9539\n",
            "Epoch 279/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1443 - acc: 0.9529 - val_loss: 0.1414 - val_acc: 0.9546\n",
            "Epoch 280/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.1554 - acc: 0.9511 - val_loss: 0.1690 - val_acc: 0.9466\n",
            "Epoch 281/1000\n",
            "54924/54924 [==============================] - 2s 34us/step - loss: 0.1408 - acc: 0.9536 - val_loss: 0.1463 - val_acc: 0.9496\n",
            "Epoch 282/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1464 - acc: 0.9528 - val_loss: 0.1667 - val_acc: 0.9511\n",
            "Epoch 283/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1455 - acc: 0.9528 - val_loss: 0.1868 - val_acc: 0.9430\n",
            "Epoch 284/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1519 - acc: 0.9526 - val_loss: 0.1403 - val_acc: 0.9546\n",
            "Epoch 285/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1459 - acc: 0.9526 - val_loss: 0.1345 - val_acc: 0.9547\n",
            "Epoch 286/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1428 - acc: 0.9526 - val_loss: 0.1561 - val_acc: 0.9518\n",
            "Epoch 287/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1415 - acc: 0.9531 - val_loss: 0.1643 - val_acc: 0.9511\n",
            "Epoch 288/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.1430 - acc: 0.9528 - val_loss: 0.1363 - val_acc: 0.9527\n",
            "Epoch 289/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1472 - acc: 0.9527 - val_loss: 0.1481 - val_acc: 0.9494\n",
            "Epoch 290/1000\n",
            "54924/54924 [==============================] - 2s 34us/step - loss: 0.1396 - acc: 0.9534 - val_loss: 0.1548 - val_acc: 0.9482\n",
            "Epoch 291/1000\n",
            "54924/54924 [==============================] - 2s 32us/step - loss: 0.1480 - acc: 0.9516 - val_loss: 0.1410 - val_acc: 0.9540\n",
            "Epoch 292/1000\n",
            "54924/54924 [==============================] - 2s 33us/step - loss: 0.1388 - acc: 0.9534 - val_loss: 0.1443 - val_acc: 0.9536\n",
            "Epoch 293/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1384 - acc: 0.9535 - val_loss: 0.1373 - val_acc: 0.9551\n",
            "Epoch 294/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1400 - acc: 0.9526 - val_loss: 0.1356 - val_acc: 0.9547\n",
            "Epoch 295/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1367 - acc: 0.9538 - val_loss: 0.1331 - val_acc: 0.9546\n",
            "Epoch 296/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1359 - acc: 0.9544 - val_loss: 0.1439 - val_acc: 0.9535\n",
            "Epoch 297/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1397 - acc: 0.9533 - val_loss: 0.1300 - val_acc: 0.9547\n",
            "Epoch 298/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1370 - acc: 0.9542 - val_loss: 0.1396 - val_acc: 0.9541\n",
            "Epoch 299/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1385 - acc: 0.9538 - val_loss: 0.2205 - val_acc: 0.9295\n",
            "Epoch 300/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1374 - acc: 0.9534 - val_loss: 0.1314 - val_acc: 0.9551\n",
            "Epoch 301/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1406 - acc: 0.9534 - val_loss: 0.1325 - val_acc: 0.9555\n",
            "Epoch 302/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1397 - acc: 0.9545 - val_loss: 0.1302 - val_acc: 0.9551\n",
            "Epoch 303/1000\n",
            "54924/54924 [==============================] - 2s 34us/step - loss: 0.1526 - acc: 0.9547 - val_loss: 0.1367 - val_acc: 0.9554\n",
            "Epoch 304/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1374 - acc: 0.9548 - val_loss: 0.1289 - val_acc: 0.9559\n",
            "Epoch 305/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1335 - acc: 0.9547 - val_loss: 0.1468 - val_acc: 0.9528\n",
            "Epoch 306/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1342 - acc: 0.9538 - val_loss: 0.1480 - val_acc: 0.9534\n",
            "Epoch 307/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1384 - acc: 0.9541 - val_loss: 0.1299 - val_acc: 0.9550\n",
            "Epoch 308/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1322 - acc: 0.9547 - val_loss: 0.1453 - val_acc: 0.9541\n",
            "Epoch 309/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1312 - acc: 0.9553 - val_loss: 0.1295 - val_acc: 0.9566\n",
            "Epoch 310/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1293 - acc: 0.9552 - val_loss: 0.1328 - val_acc: 0.9535\n",
            "Epoch 311/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1310 - acc: 0.9543 - val_loss: 0.1376 - val_acc: 0.9566\n",
            "Epoch 312/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1374 - acc: 0.9546 - val_loss: 0.1474 - val_acc: 0.9510\n",
            "Epoch 313/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1331 - acc: 0.9546 - val_loss: 0.1334 - val_acc: 0.9548\n",
            "Epoch 314/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1274 - acc: 0.9560 - val_loss: 0.1243 - val_acc: 0.9564\n",
            "Epoch 315/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1347 - acc: 0.9548 - val_loss: 0.1607 - val_acc: 0.9520\n",
            "Epoch 316/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1288 - acc: 0.9554 - val_loss: 0.1534 - val_acc: 0.9473\n",
            "Epoch 317/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.1399 - acc: 0.9533 - val_loss: 0.1236 - val_acc: 0.9572\n",
            "Epoch 318/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1293 - acc: 0.9559 - val_loss: 0.1326 - val_acc: 0.9556\n",
            "Epoch 319/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1264 - acc: 0.9556 - val_loss: 0.1302 - val_acc: 0.9562\n",
            "Epoch 320/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1285 - acc: 0.9562 - val_loss: 0.1524 - val_acc: 0.9524\n",
            "Epoch 321/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.1327 - acc: 0.9562 - val_loss: 0.1312 - val_acc: 0.9584\n",
            "Epoch 322/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1350 - acc: 0.9550 - val_loss: 0.1617 - val_acc: 0.9458\n",
            "Epoch 323/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1300 - acc: 0.9549 - val_loss: 0.1386 - val_acc: 0.9535\n",
            "Epoch 324/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1230 - acc: 0.9560 - val_loss: 0.1259 - val_acc: 0.9562\n",
            "Epoch 325/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1273 - acc: 0.9558 - val_loss: 0.1408 - val_acc: 0.9508\n",
            "Epoch 326/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1211 - acc: 0.9566 - val_loss: 0.1353 - val_acc: 0.9555\n",
            "Epoch 327/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1206 - acc: 0.9574 - val_loss: 0.1457 - val_acc: 0.9491\n",
            "Epoch 328/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1251 - acc: 0.9559 - val_loss: 0.1181 - val_acc: 0.9575\n",
            "Epoch 329/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1190 - acc: 0.9571 - val_loss: 0.1497 - val_acc: 0.9542\n",
            "Epoch 330/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1302 - acc: 0.9558 - val_loss: 0.1395 - val_acc: 0.9580\n",
            "Epoch 331/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1214 - acc: 0.9571 - val_loss: 0.1248 - val_acc: 0.9565\n",
            "Epoch 332/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1277 - acc: 0.9565 - val_loss: 0.1124 - val_acc: 0.9579\n",
            "Epoch 333/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1156 - acc: 0.9574 - val_loss: 0.1141 - val_acc: 0.9584\n",
            "Epoch 334/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1149 - acc: 0.9576 - val_loss: 0.1158 - val_acc: 0.9579\n",
            "Epoch 335/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1258 - acc: 0.9568 - val_loss: 0.1149 - val_acc: 0.9590\n",
            "Epoch 336/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1175 - acc: 0.9570 - val_loss: 0.1111 - val_acc: 0.9589\n",
            "Epoch 337/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1249 - acc: 0.9561 - val_loss: 0.1213 - val_acc: 0.9579\n",
            "Epoch 338/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1176 - acc: 0.9577 - val_loss: 0.1086 - val_acc: 0.9591\n",
            "Epoch 339/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1116 - acc: 0.9580 - val_loss: 0.1152 - val_acc: 0.9566\n",
            "Epoch 340/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1145 - acc: 0.9587 - val_loss: 0.1117 - val_acc: 0.9599\n",
            "Epoch 341/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1319 - acc: 0.9568 - val_loss: 0.1481 - val_acc: 0.9554\n",
            "Epoch 342/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.1232 - acc: 0.9580 - val_loss: 0.1210 - val_acc: 0.9567\n",
            "Epoch 343/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1822 - acc: 0.9565 - val_loss: 0.1115 - val_acc: 0.9606\n",
            "Epoch 344/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1127 - acc: 0.9589 - val_loss: 0.1462 - val_acc: 0.9523\n",
            "Epoch 345/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1225 - acc: 0.9579 - val_loss: 0.2924 - val_acc: 0.9509\n",
            "Epoch 346/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1314 - acc: 0.9575 - val_loss: 0.1137 - val_acc: 0.9597\n",
            "Epoch 347/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1117 - acc: 0.9586 - val_loss: 0.1139 - val_acc: 0.9599\n",
            "Epoch 348/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1221 - acc: 0.9579 - val_loss: 0.1576 - val_acc: 0.9543\n",
            "Epoch 349/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1144 - acc: 0.9591 - val_loss: 0.1128 - val_acc: 0.9605\n",
            "Epoch 350/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1361 - acc: 0.9567 - val_loss: 0.1391 - val_acc: 0.9582\n",
            "Epoch 351/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.1183 - acc: 0.9583 - val_loss: 0.1218 - val_acc: 0.9565\n",
            "Epoch 352/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1062 - acc: 0.9599 - val_loss: 0.1497 - val_acc: 0.9551\n",
            "Epoch 353/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1131 - acc: 0.9599 - val_loss: 0.1077 - val_acc: 0.9595\n",
            "Epoch 354/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1166 - acc: 0.9585 - val_loss: 0.2089 - val_acc: 0.9406\n",
            "Epoch 355/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1084 - acc: 0.9608 - val_loss: 0.1099 - val_acc: 0.9599\n",
            "Epoch 356/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1011 - acc: 0.9617 - val_loss: 0.0988 - val_acc: 0.9634\n",
            "Epoch 357/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1028 - acc: 0.9618 - val_loss: 0.1004 - val_acc: 0.9625\n",
            "Epoch 358/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1040 - acc: 0.9613 - val_loss: 0.1076 - val_acc: 0.9618\n",
            "Epoch 359/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1155 - acc: 0.9613 - val_loss: 0.1139 - val_acc: 0.9593\n",
            "Epoch 360/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1039 - acc: 0.9609 - val_loss: 0.2547 - val_acc: 0.9530\n",
            "Epoch 361/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1328 - acc: 0.9596 - val_loss: 0.1010 - val_acc: 0.9632\n",
            "Epoch 362/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1139 - acc: 0.9607 - val_loss: 0.2255 - val_acc: 0.9530\n",
            "Epoch 363/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1177 - acc: 0.9600 - val_loss: 0.1198 - val_acc: 0.9598\n",
            "Epoch 364/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0999 - acc: 0.9612 - val_loss: 0.1417 - val_acc: 0.9590\n",
            "Epoch 365/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1056 - acc: 0.9612 - val_loss: 0.0976 - val_acc: 0.9629\n",
            "Epoch 366/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0964 - acc: 0.9631 - val_loss: 0.0917 - val_acc: 0.9647\n",
            "Epoch 367/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0929 - acc: 0.9633 - val_loss: 0.1798 - val_acc: 0.9453\n",
            "Epoch 368/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1204 - acc: 0.9591 - val_loss: 0.0941 - val_acc: 0.9642\n",
            "Epoch 369/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0972 - acc: 0.9637 - val_loss: 0.1065 - val_acc: 0.9619\n",
            "Epoch 370/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0951 - acc: 0.9629 - val_loss: 0.1048 - val_acc: 0.9618\n",
            "Epoch 371/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0949 - acc: 0.9636 - val_loss: 0.1233 - val_acc: 0.9597\n",
            "Epoch 372/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1030 - acc: 0.9616 - val_loss: 0.1144 - val_acc: 0.9623\n",
            "Epoch 373/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1059 - acc: 0.9611 - val_loss: 0.1001 - val_acc: 0.9633\n",
            "Epoch 374/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1049 - acc: 0.9624 - val_loss: 0.0846 - val_acc: 0.9659\n",
            "Epoch 375/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0996 - acc: 0.9631 - val_loss: 0.0895 - val_acc: 0.9671\n",
            "Epoch 376/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0998 - acc: 0.9631 - val_loss: 0.2093 - val_acc: 0.9456\n",
            "Epoch 377/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0930 - acc: 0.9646 - val_loss: 0.0842 - val_acc: 0.9668\n",
            "Epoch 378/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1020 - acc: 0.9640 - val_loss: 0.1508 - val_acc: 0.9586\n",
            "Epoch 379/1000\n",
            "54924/54924 [==============================] - 2s 34us/step - loss: 0.0909 - acc: 0.9651 - val_loss: 0.1062 - val_acc: 0.9614\n",
            "Epoch 380/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0904 - acc: 0.9652 - val_loss: 0.1270 - val_acc: 0.9597\n",
            "Epoch 381/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1112 - acc: 0.9621 - val_loss: 0.1064 - val_acc: 0.9641\n",
            "Epoch 382/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0987 - acc: 0.9638 - val_loss: 0.1014 - val_acc: 0.9621\n",
            "Epoch 383/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0976 - acc: 0.9637 - val_loss: 0.0928 - val_acc: 0.9637\n",
            "Epoch 384/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0985 - acc: 0.9649 - val_loss: 0.0864 - val_acc: 0.9659\n",
            "Epoch 385/1000\n",
            "54924/54924 [==============================] - 2s 34us/step - loss: 0.1373 - acc: 0.9583 - val_loss: 0.0883 - val_acc: 0.9628\n",
            "Epoch 386/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0953 - acc: 0.9633 - val_loss: 0.0894 - val_acc: 0.9638\n",
            "Epoch 387/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0831 - acc: 0.9668 - val_loss: 0.0801 - val_acc: 0.9672\n",
            "Epoch 388/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0794 - acc: 0.9676 - val_loss: 0.1092 - val_acc: 0.9630\n",
            "Epoch 389/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0907 - acc: 0.9663 - val_loss: 0.0918 - val_acc: 0.9649\n",
            "Epoch 390/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0843 - acc: 0.9677 - val_loss: 0.1480 - val_acc: 0.9599\n",
            "Epoch 391/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0923 - acc: 0.9668 - val_loss: 0.0953 - val_acc: 0.9672\n",
            "Epoch 392/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0894 - acc: 0.9664 - val_loss: 0.1047 - val_acc: 0.9677\n",
            "Epoch 393/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0780 - acc: 0.9690 - val_loss: 0.0931 - val_acc: 0.9664\n",
            "Epoch 394/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0843 - acc: 0.9687 - val_loss: 0.0749 - val_acc: 0.9693\n",
            "Epoch 395/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0921 - acc: 0.9673 - val_loss: 0.0878 - val_acc: 0.9682\n",
            "Epoch 396/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0785 - acc: 0.9698 - val_loss: 0.0685 - val_acc: 0.9693\n",
            "Epoch 397/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0864 - acc: 0.9692 - val_loss: 0.0749 - val_acc: 0.9692\n",
            "Epoch 398/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0742 - acc: 0.9715 - val_loss: 0.0917 - val_acc: 0.9693\n",
            "Epoch 399/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0799 - acc: 0.9719 - val_loss: 0.0781 - val_acc: 0.9699\n",
            "Epoch 400/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0749 - acc: 0.9733 - val_loss: 0.2233 - val_acc: 0.9345\n",
            "Epoch 401/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0806 - acc: 0.9732 - val_loss: 0.1173 - val_acc: 0.9613\n",
            "Epoch 402/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0869 - acc: 0.9735 - val_loss: 0.0894 - val_acc: 0.9759\n",
            "Epoch 403/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0719 - acc: 0.9754 - val_loss: 0.0744 - val_acc: 0.9754\n",
            "Epoch 404/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0746 - acc: 0.9759 - val_loss: 0.1027 - val_acc: 0.9710\n",
            "Epoch 405/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0890 - acc: 0.9747 - val_loss: 0.0679 - val_acc: 0.9797\n",
            "Epoch 406/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0720 - acc: 0.9766 - val_loss: 0.0670 - val_acc: 0.9788\n",
            "Epoch 407/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0735 - acc: 0.9768 - val_loss: 0.0586 - val_acc: 0.9814\n",
            "Epoch 408/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0667 - acc: 0.9784 - val_loss: 0.0823 - val_acc: 0.9725\n",
            "Epoch 409/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0675 - acc: 0.9782 - val_loss: 0.0602 - val_acc: 0.9817\n",
            "Epoch 410/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0805 - acc: 0.9771 - val_loss: 0.0961 - val_acc: 0.9720\n",
            "Epoch 411/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0793 - acc: 0.9769 - val_loss: 0.0719 - val_acc: 0.9824\n",
            "Epoch 412/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0730 - acc: 0.9786 - val_loss: 0.0997 - val_acc: 0.9731\n",
            "Epoch 413/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1331 - acc: 0.9742 - val_loss: 0.0727 - val_acc: 0.9764\n",
            "Epoch 414/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0701 - acc: 0.9801 - val_loss: 0.0668 - val_acc: 0.9842\n",
            "Epoch 415/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0716 - acc: 0.9806 - val_loss: 0.0570 - val_acc: 0.9854\n",
            "Epoch 416/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0599 - acc: 0.9827 - val_loss: 0.1234 - val_acc: 0.9733\n",
            "Epoch 417/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0776 - acc: 0.9802 - val_loss: 0.0601 - val_acc: 0.9801\n",
            "Epoch 418/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0612 - acc: 0.9831 - val_loss: 0.0596 - val_acc: 0.9816\n",
            "Epoch 419/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0570 - acc: 0.9836 - val_loss: 0.0409 - val_acc: 0.9873\n",
            "Epoch 420/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0652 - acc: 0.9828 - val_loss: 0.0421 - val_acc: 0.9883\n",
            "Epoch 421/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0729 - acc: 0.9808 - val_loss: 0.0607 - val_acc: 0.9824\n",
            "Epoch 422/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0586 - acc: 0.9852 - val_loss: 0.1316 - val_acc: 0.9650\n",
            "Epoch 423/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0995 - acc: 0.9761 - val_loss: 0.1971 - val_acc: 0.9457\n",
            "Epoch 424/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0700 - acc: 0.9830 - val_loss: 0.0524 - val_acc: 0.9830\n",
            "Epoch 425/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0607 - acc: 0.9849 - val_loss: 0.0665 - val_acc: 0.9870\n",
            "Epoch 426/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0643 - acc: 0.9844 - val_loss: 0.0843 - val_acc: 0.9809\n",
            "Epoch 427/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1368 - acc: 0.9804 - val_loss: 0.0558 - val_acc: 0.9864\n",
            "Epoch 428/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0824 - acc: 0.9818 - val_loss: 0.1060 - val_acc: 0.9764\n",
            "Epoch 429/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0637 - acc: 0.9865 - val_loss: 0.1040 - val_acc: 0.9712\n",
            "Epoch 430/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0775 - acc: 0.9840 - val_loss: 0.0430 - val_acc: 0.9894\n",
            "Epoch 431/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0612 - acc: 0.9851 - val_loss: 0.0551 - val_acc: 0.9897\n",
            "Epoch 432/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0951 - acc: 0.9840 - val_loss: 0.0993 - val_acc: 0.9768\n",
            "Epoch 433/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0557 - acc: 0.9878 - val_loss: 0.0686 - val_acc: 0.9862\n",
            "Epoch 434/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0634 - acc: 0.9863 - val_loss: 0.0946 - val_acc: 0.9754\n",
            "Epoch 435/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0687 - acc: 0.9858 - val_loss: 0.0830 - val_acc: 0.9854\n",
            "Epoch 436/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0663 - acc: 0.9849 - val_loss: 0.1053 - val_acc: 0.9771\n",
            "Epoch 437/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0628 - acc: 0.9855 - val_loss: 0.0445 - val_acc: 0.9890\n",
            "Epoch 438/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0484 - acc: 0.9889 - val_loss: 0.0442 - val_acc: 0.9926\n",
            "Epoch 439/1000\n",
            "54924/54924 [==============================] - 2s 32us/step - loss: 0.0520 - acc: 0.9879 - val_loss: 0.0442 - val_acc: 0.9919\n",
            "Epoch 440/1000\n",
            "54924/54924 [==============================] - 2s 32us/step - loss: 0.0558 - acc: 0.9868 - val_loss: 0.0534 - val_acc: 0.9885\n",
            "Epoch 441/1000\n",
            "54924/54924 [==============================] - 2s 32us/step - loss: 0.0628 - acc: 0.9865 - val_loss: 0.0514 - val_acc: 0.9918\n",
            "Epoch 442/1000\n",
            "54924/54924 [==============================] - 2s 33us/step - loss: 0.0558 - acc: 0.9879 - val_loss: 0.0469 - val_acc: 0.9927\n",
            "Epoch 443/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0761 - acc: 0.9848 - val_loss: 0.0690 - val_acc: 0.9881\n",
            "Epoch 444/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0639 - acc: 0.9859 - val_loss: 0.0404 - val_acc: 0.9940\n",
            "Epoch 445/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0522 - acc: 0.9895 - val_loss: 0.0670 - val_acc: 0.9833\n",
            "Epoch 446/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0948 - acc: 0.9857 - val_loss: 0.0782 - val_acc: 0.9858\n",
            "Epoch 447/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0622 - acc: 0.9875 - val_loss: 0.0711 - val_acc: 0.9848\n",
            "Epoch 448/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0648 - acc: 0.9880 - val_loss: 0.3932 - val_acc: 0.9261\n",
            "Epoch 449/1000\n",
            "54924/54924 [==============================] - 2s 34us/step - loss: 0.0841 - acc: 0.9855 - val_loss: 0.0555 - val_acc: 0.9886\n",
            "Epoch 450/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0596 - acc: 0.9888 - val_loss: 0.0390 - val_acc: 0.9888\n",
            "Epoch 451/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0601 - acc: 0.9883 - val_loss: 0.0502 - val_acc: 0.9881\n",
            "Epoch 452/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0695 - acc: 0.9865 - val_loss: 0.0468 - val_acc: 0.9912\n",
            "Epoch 453/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0735 - acc: 0.9881 - val_loss: 0.0501 - val_acc: 0.9926\n",
            "Epoch 454/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0799 - acc: 0.9862 - val_loss: 0.0661 - val_acc: 0.9851\n",
            "Epoch 455/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0635 - acc: 0.9879 - val_loss: 0.0349 - val_acc: 0.9921\n",
            "Epoch 456/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0706 - acc: 0.9882 - val_loss: 0.0679 - val_acc: 0.9841\n",
            "Epoch 457/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0591 - acc: 0.9889 - val_loss: 0.0474 - val_acc: 0.9895\n",
            "Epoch 458/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0641 - acc: 0.9877 - val_loss: 0.0577 - val_acc: 0.9904\n",
            "Epoch 459/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0723 - acc: 0.9858 - val_loss: 0.0561 - val_acc: 0.9864\n",
            "Epoch 460/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0665 - acc: 0.9869 - val_loss: 0.1032 - val_acc: 0.9785\n",
            "Epoch 461/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0410 - acc: 0.9912 - val_loss: 0.0400 - val_acc: 0.9927\n",
            "Epoch 462/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0491 - acc: 0.9910 - val_loss: 0.0533 - val_acc: 0.9874\n",
            "Epoch 463/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0528 - acc: 0.9897 - val_loss: 0.0351 - val_acc: 0.9952\n",
            "Epoch 464/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0523 - acc: 0.9905 - val_loss: 0.0794 - val_acc: 0.9829\n",
            "Epoch 465/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0594 - acc: 0.9891 - val_loss: 0.0536 - val_acc: 0.9921\n",
            "Epoch 466/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0560 - acc: 0.9900 - val_loss: 0.0837 - val_acc: 0.9911\n",
            "Epoch 467/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0848 - acc: 0.9859 - val_loss: 0.0376 - val_acc: 0.9949\n",
            "Epoch 468/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0800 - acc: 0.9845 - val_loss: 0.0508 - val_acc: 0.9888\n",
            "Epoch 469/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0725 - acc: 0.9856 - val_loss: 0.0908 - val_acc: 0.9851\n",
            "Epoch 470/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0563 - acc: 0.9891 - val_loss: 0.0506 - val_acc: 0.9891\n",
            "Epoch 471/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0468 - acc: 0.9902 - val_loss: 0.0349 - val_acc: 0.9943\n",
            "Epoch 472/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0466 - acc: 0.9906 - val_loss: 0.0367 - val_acc: 0.9955\n",
            "Epoch 473/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0819 - acc: 0.9880 - val_loss: 0.0504 - val_acc: 0.9934\n",
            "Epoch 474/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0496 - acc: 0.9920 - val_loss: 0.2491 - val_acc: 0.9339\n",
            "Epoch 475/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0543 - acc: 0.9908 - val_loss: 0.0511 - val_acc: 0.9906\n",
            "Epoch 476/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0806 - acc: 0.9838 - val_loss: 0.0471 - val_acc: 0.9956\n",
            "Epoch 477/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0522 - acc: 0.9900 - val_loss: 0.0826 - val_acc: 0.9856\n",
            "Epoch 478/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0603 - acc: 0.9878 - val_loss: 0.0355 - val_acc: 0.9931\n",
            "Epoch 479/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0493 - acc: 0.9906 - val_loss: 0.0435 - val_acc: 0.9915\n",
            "Epoch 480/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0418 - acc: 0.9913 - val_loss: 0.0544 - val_acc: 0.9881\n",
            "Epoch 481/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0518 - acc: 0.9906 - val_loss: 0.0537 - val_acc: 0.9875\n",
            "Epoch 482/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0400 - acc: 0.9924 - val_loss: 0.1421 - val_acc: 0.9613\n",
            "Epoch 483/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0540 - acc: 0.9893 - val_loss: 0.0646 - val_acc: 0.9800\n",
            "Epoch 484/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0389 - acc: 0.9922 - val_loss: 0.0567 - val_acc: 0.9887\n",
            "Epoch 485/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0815 - acc: 0.9872 - val_loss: 0.1272 - val_acc: 0.9806\n",
            "Epoch 486/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0576 - acc: 0.9899 - val_loss: 0.0729 - val_acc: 0.9850\n",
            "Epoch 487/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0852 - acc: 0.9846 - val_loss: 0.0746 - val_acc: 0.9901\n",
            "Epoch 488/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0478 - acc: 0.9899 - val_loss: 0.1172 - val_acc: 0.9798\n",
            "Epoch 489/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1946 - acc: 0.9814 - val_loss: 0.0534 - val_acc: 0.9920\n",
            "Epoch 490/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0521 - acc: 0.9906 - val_loss: 0.0272 - val_acc: 0.9969\n",
            "Epoch 491/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0488 - acc: 0.9914 - val_loss: 0.0310 - val_acc: 0.9964\n",
            "Epoch 492/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0583 - acc: 0.9911 - val_loss: 0.0689 - val_acc: 0.9867\n",
            "Epoch 493/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0799 - acc: 0.9885 - val_loss: 0.0528 - val_acc: 0.9907\n",
            "Epoch 494/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0401 - acc: 0.9932 - val_loss: 0.0223 - val_acc: 0.9970\n",
            "Epoch 495/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0721 - acc: 0.9884 - val_loss: 0.0310 - val_acc: 0.9946\n",
            "Epoch 496/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0365 - acc: 0.9941 - val_loss: 0.0486 - val_acc: 0.9908\n",
            "Epoch 497/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0405 - acc: 0.9930 - val_loss: 0.0401 - val_acc: 0.9921\n",
            "Epoch 498/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0495 - acc: 0.9910 - val_loss: 0.0266 - val_acc: 0.9972\n",
            "Epoch 499/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0410 - acc: 0.9921 - val_loss: 0.0272 - val_acc: 0.9968\n",
            "Epoch 500/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0403 - acc: 0.9928 - val_loss: 0.0226 - val_acc: 0.9969\n",
            "Epoch 501/1000\n",
            "54924/54924 [==============================] - 2s 34us/step - loss: 0.0659 - acc: 0.9888 - val_loss: 0.2718 - val_acc: 0.9705\n",
            "Epoch 502/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0558 - acc: 0.9915 - val_loss: 0.0421 - val_acc: 0.9951\n",
            "Epoch 503/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0605 - acc: 0.9909 - val_loss: 0.0332 - val_acc: 0.9950\n",
            "Epoch 504/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0499 - acc: 0.9920 - val_loss: 0.0341 - val_acc: 0.9963\n",
            "Epoch 505/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0495 - acc: 0.9926 - val_loss: 0.1173 - val_acc: 0.9838\n",
            "Epoch 506/1000\n",
            "54924/54924 [==============================] - 2s 34us/step - loss: 0.0442 - acc: 0.9926 - val_loss: 0.0327 - val_acc: 0.9961\n",
            "Epoch 507/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0793 - acc: 0.9885 - val_loss: 0.0474 - val_acc: 0.9918\n",
            "Epoch 508/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1080 - acc: 0.9855 - val_loss: 0.0347 - val_acc: 0.9950\n",
            "Epoch 509/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0361 - acc: 0.9940 - val_loss: 0.0220 - val_acc: 0.9967\n",
            "Epoch 510/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0390 - acc: 0.9927 - val_loss: 0.0394 - val_acc: 0.9935\n",
            "Epoch 511/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0609 - acc: 0.9886 - val_loss: 0.0433 - val_acc: 0.9932\n",
            "Epoch 512/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0652 - acc: 0.9897 - val_loss: 0.0239 - val_acc: 0.9966\n",
            "Epoch 513/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0430 - acc: 0.9921 - val_loss: 0.0654 - val_acc: 0.9879\n",
            "Epoch 514/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0731 - acc: 0.9895 - val_loss: 0.0374 - val_acc: 0.9948\n",
            "Epoch 515/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0457 - acc: 0.9924 - val_loss: 0.1654 - val_acc: 0.9688\n",
            "Epoch 516/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0553 - acc: 0.9909 - val_loss: 0.0439 - val_acc: 0.9931\n",
            "Epoch 517/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0394 - acc: 0.9937 - val_loss: 0.0475 - val_acc: 0.9919\n",
            "Epoch 518/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0672 - acc: 0.9895 - val_loss: 0.0634 - val_acc: 0.9871\n",
            "Epoch 519/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0510 - acc: 0.9915 - val_loss: 0.0319 - val_acc: 0.9967\n",
            "Epoch 520/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.1129 - acc: 0.9858 - val_loss: 0.0403 - val_acc: 0.9961\n",
            "Epoch 521/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0938 - acc: 0.9870 - val_loss: 0.0966 - val_acc: 0.9816\n",
            "Epoch 522/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0735 - acc: 0.9887 - val_loss: 0.0366 - val_acc: 0.9961\n",
            "Epoch 523/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0546 - acc: 0.9916 - val_loss: 0.0349 - val_acc: 0.9956\n",
            "Epoch 524/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0479 - acc: 0.9903 - val_loss: 0.0470 - val_acc: 0.9934\n",
            "Epoch 525/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0409 - acc: 0.9920 - val_loss: 0.0286 - val_acc: 0.9961\n",
            "Epoch 526/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0317 - acc: 0.9939 - val_loss: 0.0482 - val_acc: 0.9900\n",
            "Epoch 527/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0317 - acc: 0.9939 - val_loss: 0.0441 - val_acc: 0.9927\n",
            "Epoch 528/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0600 - acc: 0.9901 - val_loss: 0.0705 - val_acc: 0.9871\n",
            "Epoch 529/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0416 - acc: 0.9925 - val_loss: 0.0738 - val_acc: 0.9879\n",
            "Epoch 530/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0448 - acc: 0.9920 - val_loss: 0.0250 - val_acc: 0.9971\n",
            "Epoch 531/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0579 - acc: 0.9913 - val_loss: 0.5286 - val_acc: 0.9608\n",
            "Epoch 532/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.1602 - acc: 0.9838 - val_loss: 0.0779 - val_acc: 0.9869\n",
            "Epoch 533/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0369 - acc: 0.9930 - val_loss: 0.0273 - val_acc: 0.9966\n",
            "Epoch 534/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0337 - acc: 0.9931 - val_loss: 0.0843 - val_acc: 0.9788\n",
            "Epoch 535/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0335 - acc: 0.9936 - val_loss: 0.0296 - val_acc: 0.9937\n",
            "Epoch 536/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0405 - acc: 0.9927 - val_loss: 0.0204 - val_acc: 0.9969\n",
            "Epoch 537/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0494 - acc: 0.9923 - val_loss: 0.0432 - val_acc: 0.9921\n",
            "Epoch 538/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0639 - acc: 0.9893 - val_loss: 0.0384 - val_acc: 0.9941\n",
            "Epoch 539/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0516 - acc: 0.9913 - val_loss: 0.0371 - val_acc: 0.9941\n",
            "Epoch 540/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0559 - acc: 0.9916 - val_loss: 0.0313 - val_acc: 0.9967\n",
            "Epoch 541/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0676 - acc: 0.9896 - val_loss: 0.0276 - val_acc: 0.9973\n",
            "Epoch 542/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0522 - acc: 0.9918 - val_loss: 0.0402 - val_acc: 0.9937\n",
            "Epoch 543/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0621 - acc: 0.9905 - val_loss: 0.1529 - val_acc: 0.9804\n",
            "Epoch 544/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0592 - acc: 0.9907 - val_loss: 0.0656 - val_acc: 0.9888\n",
            "Epoch 545/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0405 - acc: 0.9944 - val_loss: 0.0319 - val_acc: 0.9961\n",
            "Epoch 546/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0512 - acc: 0.9914 - val_loss: 0.0328 - val_acc: 0.9958\n",
            "Epoch 547/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0584 - acc: 0.9919 - val_loss: 0.0364 - val_acc: 0.9964\n",
            "Epoch 548/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0428 - acc: 0.9936 - val_loss: 0.0256 - val_acc: 0.9959\n",
            "Epoch 549/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0448 - acc: 0.9926 - val_loss: 0.0955 - val_acc: 0.9790\n",
            "Epoch 550/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0497 - acc: 0.9919 - val_loss: 0.0404 - val_acc: 0.9935\n",
            "Epoch 551/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0425 - acc: 0.9932 - val_loss: 0.0498 - val_acc: 0.9906\n",
            "Epoch 552/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0383 - acc: 0.9946 - val_loss: 0.0636 - val_acc: 0.9888\n",
            "Epoch 553/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0461 - acc: 0.9918 - val_loss: 0.0666 - val_acc: 0.9889\n",
            "Epoch 554/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0511 - acc: 0.9922 - val_loss: 0.0291 - val_acc: 0.9968\n",
            "Epoch 555/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0560 - acc: 0.9914 - val_loss: 0.0461 - val_acc: 0.9921\n",
            "Epoch 556/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0469 - acc: 0.9933 - val_loss: 0.0205 - val_acc: 0.9972\n",
            "Epoch 557/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0784 - acc: 0.9879 - val_loss: 0.0343 - val_acc: 0.9965\n",
            "Epoch 558/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0360 - acc: 0.9940 - val_loss: 0.0255 - val_acc: 0.9972\n",
            "Epoch 559/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0674 - acc: 0.9914 - val_loss: 0.0714 - val_acc: 0.9896\n",
            "Epoch 560/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0610 - acc: 0.9901 - val_loss: 0.0553 - val_acc: 0.9891\n",
            "Epoch 561/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0434 - acc: 0.9929 - val_loss: 0.0821 - val_acc: 0.9825\n",
            "Epoch 562/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0508 - acc: 0.9917 - val_loss: 0.0590 - val_acc: 0.9900\n",
            "Epoch 563/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0764 - acc: 0.9878 - val_loss: 0.0445 - val_acc: 0.9933\n",
            "Epoch 564/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0524 - acc: 0.9910 - val_loss: 0.0511 - val_acc: 0.9914\n",
            "Epoch 565/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0385 - acc: 0.9938 - val_loss: 0.0199 - val_acc: 0.9971\n",
            "Epoch 566/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0776 - acc: 0.9908 - val_loss: 0.0579 - val_acc: 0.9916\n",
            "Epoch 567/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0464 - acc: 0.9921 - val_loss: 0.1332 - val_acc: 0.9699\n",
            "Epoch 568/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0676 - acc: 0.9895 - val_loss: 0.1336 - val_acc: 0.9797\n",
            "Epoch 569/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0671 - acc: 0.9907 - val_loss: 0.0306 - val_acc: 0.9970\n",
            "Epoch 570/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0633 - acc: 0.9905 - val_loss: 0.0395 - val_acc: 0.9950\n",
            "Epoch 571/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0690 - acc: 0.9897 - val_loss: 0.0336 - val_acc: 0.9959\n",
            "Epoch 572/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0510 - acc: 0.9920 - val_loss: 0.0392 - val_acc: 0.9945\n",
            "Epoch 573/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0713 - acc: 0.9891 - val_loss: 0.0293 - val_acc: 0.9974\n",
            "Epoch 574/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0471 - acc: 0.9921 - val_loss: 0.0199 - val_acc: 0.9972\n",
            "Epoch 575/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0362 - acc: 0.9945 - val_loss: 0.0226 - val_acc: 0.9976\n",
            "Epoch 576/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0496 - acc: 0.9921 - val_loss: 0.0267 - val_acc: 0.9972\n",
            "Epoch 577/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0647 - acc: 0.9895 - val_loss: 0.0313 - val_acc: 0.9964\n",
            "Epoch 578/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0281 - acc: 0.9952 - val_loss: 0.0710 - val_acc: 0.9877\n",
            "Epoch 579/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0506 - acc: 0.9929 - val_loss: 0.0298 - val_acc: 0.9966\n",
            "Epoch 580/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0708 - acc: 0.9885 - val_loss: 0.0508 - val_acc: 0.9920\n",
            "Epoch 581/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0509 - acc: 0.9931 - val_loss: 0.2191 - val_acc: 0.9757\n",
            "Epoch 582/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0391 - acc: 0.9943 - val_loss: 0.0444 - val_acc: 0.9928\n",
            "Epoch 583/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0287 - acc: 0.9950 - val_loss: 0.0503 - val_acc: 0.9904\n",
            "Epoch 584/1000\n",
            "54924/54924 [==============================] - 2s 33us/step - loss: 0.0524 - acc: 0.9921 - val_loss: 0.0351 - val_acc: 0.9953\n",
            "Epoch 585/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0529 - acc: 0.9908 - val_loss: 0.0569 - val_acc: 0.9902\n",
            "Epoch 586/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0524 - acc: 0.9927 - val_loss: 0.1964 - val_acc: 0.9774\n",
            "Epoch 587/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0668 - acc: 0.9916 - val_loss: 0.0789 - val_acc: 0.9873\n",
            "Epoch 588/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0595 - acc: 0.9898 - val_loss: 0.0303 - val_acc: 0.9959\n",
            "Epoch 589/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0461 - acc: 0.9930 - val_loss: 0.0361 - val_acc: 0.9952\n",
            "Epoch 590/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0418 - acc: 0.9930 - val_loss: 0.0424 - val_acc: 0.9932\n",
            "Epoch 591/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0673 - acc: 0.9913 - val_loss: 0.1052 - val_acc: 0.9858\n",
            "Epoch 592/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0793 - acc: 0.9890 - val_loss: 0.1500 - val_acc: 0.9701\n",
            "Epoch 593/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0447 - acc: 0.9937 - val_loss: 0.0168 - val_acc: 0.9975\n",
            "Epoch 594/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0492 - acc: 0.9935 - val_loss: 0.0288 - val_acc: 0.9967\n",
            "Epoch 595/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0879 - acc: 0.9881 - val_loss: 0.0792 - val_acc: 0.9877\n",
            "Epoch 596/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0510 - acc: 0.9929 - val_loss: 0.1972 - val_acc: 0.9567\n",
            "Epoch 597/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0714 - acc: 0.9906 - val_loss: 0.0619 - val_acc: 0.9923\n",
            "Epoch 598/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0458 - acc: 0.9931 - val_loss: 0.0237 - val_acc: 0.9973\n",
            "Epoch 599/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0454 - acc: 0.9930 - val_loss: 0.1559 - val_acc: 0.9800\n",
            "Epoch 600/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0791 - acc: 0.9876 - val_loss: 0.0368 - val_acc: 0.9951\n",
            "Epoch 601/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0512 - acc: 0.9910 - val_loss: 0.0684 - val_acc: 0.9878\n",
            "Epoch 602/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0345 - acc: 0.9938 - val_loss: 0.0166 - val_acc: 0.9976\n",
            "Epoch 603/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0354 - acc: 0.9940 - val_loss: 0.0624 - val_acc: 0.9897\n",
            "Epoch 604/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0926 - acc: 0.9898 - val_loss: 0.4504 - val_acc: 0.9649\n",
            "Epoch 605/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0800 - acc: 0.9904 - val_loss: 0.0233 - val_acc: 0.9975\n",
            "Epoch 606/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0413 - acc: 0.9934 - val_loss: 0.0584 - val_acc: 0.9909\n",
            "Epoch 607/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0484 - acc: 0.9927 - val_loss: 0.0790 - val_acc: 0.9833\n",
            "Epoch 608/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0345 - acc: 0.9943 - val_loss: 0.0287 - val_acc: 0.9967\n",
            "Epoch 609/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0470 - acc: 0.9918 - val_loss: 0.0720 - val_acc: 0.9875\n",
            "Epoch 610/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0400 - acc: 0.9939 - val_loss: 0.0355 - val_acc: 0.9931\n",
            "Epoch 611/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0403 - acc: 0.9926 - val_loss: 0.1323 - val_acc: 0.9807\n",
            "Epoch 612/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0442 - acc: 0.9922 - val_loss: 0.0212 - val_acc: 0.9977\n",
            "Epoch 613/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0357 - acc: 0.9939 - val_loss: 0.0373 - val_acc: 0.9947\n",
            "Epoch 614/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0454 - acc: 0.9921 - val_loss: 0.0341 - val_acc: 0.9966\n",
            "Epoch 615/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0436 - acc: 0.9924 - val_loss: 0.0286 - val_acc: 0.9968\n",
            "Epoch 616/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0495 - acc: 0.9925 - val_loss: 0.0392 - val_acc: 0.9927\n",
            "Epoch 617/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0655 - acc: 0.9903 - val_loss: 0.0643 - val_acc: 0.9894\n",
            "Epoch 618/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0716 - acc: 0.9901 - val_loss: 0.0331 - val_acc: 0.9945\n",
            "Epoch 619/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0561 - acc: 0.9918 - val_loss: 0.0415 - val_acc: 0.9940\n",
            "Epoch 620/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0333 - acc: 0.9948 - val_loss: 0.0374 - val_acc: 0.9958\n",
            "Epoch 621/1000\n",
            "54924/54924 [==============================] - 2s 33us/step - loss: 0.0588 - acc: 0.9910 - val_loss: 0.2414 - val_acc: 0.9743\n",
            "Epoch 622/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0590 - acc: 0.9911 - val_loss: 0.0917 - val_acc: 0.9862\n",
            "Epoch 623/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0376 - acc: 0.9944 - val_loss: 0.0263 - val_acc: 0.9968\n",
            "Epoch 624/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0479 - acc: 0.9925 - val_loss: 0.0287 - val_acc: 0.9969\n",
            "Epoch 625/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0570 - acc: 0.9907 - val_loss: 0.1556 - val_acc: 0.9800\n",
            "Epoch 626/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0592 - acc: 0.9921 - val_loss: 0.0317 - val_acc: 0.9968\n",
            "Epoch 627/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0370 - acc: 0.9941 - val_loss: 0.0325 - val_acc: 0.9963\n",
            "Epoch 628/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0391 - acc: 0.9940 - val_loss: 0.0289 - val_acc: 0.9961\n",
            "Epoch 629/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0476 - acc: 0.9922 - val_loss: 0.0676 - val_acc: 0.9890\n",
            "Epoch 630/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0385 - acc: 0.9931 - val_loss: 0.0219 - val_acc: 0.9960\n",
            "Epoch 631/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0802 - acc: 0.9891 - val_loss: 0.0520 - val_acc: 0.9940\n",
            "Epoch 632/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0500 - acc: 0.9926 - val_loss: 0.0723 - val_acc: 0.9887\n",
            "Epoch 633/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0838 - acc: 0.9896 - val_loss: 0.1144 - val_acc: 0.9841\n",
            "Epoch 634/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0389 - acc: 0.9942 - val_loss: 0.3254 - val_acc: 0.9703\n",
            "Epoch 635/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0470 - acc: 0.9934 - val_loss: 0.0353 - val_acc: 0.9956\n",
            "Epoch 636/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0435 - acc: 0.9934 - val_loss: 0.0178 - val_acc: 0.9979\n",
            "Epoch 637/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0623 - acc: 0.9911 - val_loss: 0.0373 - val_acc: 0.9955\n",
            "Epoch 638/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0630 - acc: 0.9902 - val_loss: 0.0174 - val_acc: 0.9983\n",
            "Epoch 639/1000\n",
            "54924/54924 [==============================] - 2s 33us/step - loss: 0.0559 - acc: 0.9909 - val_loss: 0.0180 - val_acc: 0.9978\n",
            "Epoch 640/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0316 - acc: 0.9951 - val_loss: 0.0487 - val_acc: 0.9924\n",
            "Epoch 641/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0672 - acc: 0.9883 - val_loss: 0.0493 - val_acc: 0.9925\n",
            "Epoch 642/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0356 - acc: 0.9945 - val_loss: 0.0362 - val_acc: 0.9958\n",
            "Epoch 643/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0387 - acc: 0.9931 - val_loss: 0.0357 - val_acc: 0.9955\n",
            "Epoch 644/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0352 - acc: 0.9950 - val_loss: 0.1013 - val_acc: 0.9847\n",
            "Epoch 645/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0398 - acc: 0.9942 - val_loss: 0.0313 - val_acc: 0.9961\n",
            "Epoch 646/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0395 - acc: 0.9943 - val_loss: 0.0297 - val_acc: 0.9964\n",
            "Epoch 647/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1057 - acc: 0.9841 - val_loss: 0.0227 - val_acc: 0.9975\n",
            "Epoch 648/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0941 - acc: 0.9898 - val_loss: 0.0331 - val_acc: 0.9964\n",
            "Epoch 649/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0610 - acc: 0.9906 - val_loss: 0.0983 - val_acc: 0.9859\n",
            "Epoch 650/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0348 - acc: 0.9944 - val_loss: 0.0212 - val_acc: 0.9977\n",
            "Epoch 651/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0303 - acc: 0.9949 - val_loss: 0.0538 - val_acc: 0.9915\n",
            "Epoch 652/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0290 - acc: 0.9948 - val_loss: 0.0259 - val_acc: 0.9958\n",
            "Epoch 653/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0343 - acc: 0.9949 - val_loss: 0.0415 - val_acc: 0.9934\n",
            "Epoch 654/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0379 - acc: 0.9938 - val_loss: 0.0226 - val_acc: 0.9961\n",
            "Epoch 655/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1461 - acc: 0.9851 - val_loss: 0.0455 - val_acc: 0.9940\n",
            "Epoch 656/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0538 - acc: 0.9925 - val_loss: 0.0245 - val_acc: 0.9976\n",
            "Epoch 657/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0436 - acc: 0.9940 - val_loss: 0.0827 - val_acc: 0.9875\n",
            "Epoch 658/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0494 - acc: 0.9929 - val_loss: 0.0468 - val_acc: 0.9931\n",
            "Epoch 659/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0303 - acc: 0.9948 - val_loss: 0.0447 - val_acc: 0.9932\n",
            "Epoch 660/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0331 - acc: 0.9950 - val_loss: 0.0635 - val_acc: 0.9895\n",
            "Epoch 661/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0671 - acc: 0.9918 - val_loss: 0.0329 - val_acc: 0.9962\n",
            "Epoch 662/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0607 - acc: 0.9904 - val_loss: 0.0405 - val_acc: 0.9941\n",
            "Epoch 663/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0496 - acc: 0.9937 - val_loss: 0.0397 - val_acc: 0.9941\n",
            "Epoch 664/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0356 - acc: 0.9943 - val_loss: 0.0610 - val_acc: 0.9890\n",
            "Epoch 665/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0720 - acc: 0.9886 - val_loss: 0.0717 - val_acc: 0.9876\n",
            "Epoch 666/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0498 - acc: 0.9933 - val_loss: 0.0175 - val_acc: 0.9977\n",
            "Epoch 667/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0307 - acc: 0.9953 - val_loss: 0.1080 - val_acc: 0.9843\n",
            "Epoch 668/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0505 - acc: 0.9929 - val_loss: 0.1008 - val_acc: 0.9871\n",
            "Epoch 669/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0428 - acc: 0.9931 - val_loss: 0.0494 - val_acc: 0.9921\n",
            "Epoch 670/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0534 - acc: 0.9911 - val_loss: 0.1025 - val_acc: 0.9838\n",
            "Epoch 671/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0343 - acc: 0.9944 - val_loss: 0.0254 - val_acc: 0.9969\n",
            "Epoch 672/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0585 - acc: 0.9907 - val_loss: 0.0282 - val_acc: 0.9970\n",
            "Epoch 673/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0433 - acc: 0.9929 - val_loss: 0.0239 - val_acc: 0.9975\n",
            "Epoch 674/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0869 - acc: 0.9875 - val_loss: 0.0377 - val_acc: 0.9963\n",
            "Epoch 675/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0709 - acc: 0.9884 - val_loss: 0.0534 - val_acc: 0.9910\n",
            "Epoch 676/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0479 - acc: 0.9928 - val_loss: 0.0449 - val_acc: 0.9925\n",
            "Epoch 677/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0390 - acc: 0.9942 - val_loss: 0.0169 - val_acc: 0.9978\n",
            "Epoch 678/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0359 - acc: 0.9937 - val_loss: 0.2001 - val_acc: 0.9534\n",
            "Epoch 679/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0528 - acc: 0.9914 - val_loss: 0.0155 - val_acc: 0.9984\n",
            "Epoch 680/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0392 - acc: 0.9936 - val_loss: 0.0209 - val_acc: 0.9977\n",
            "Epoch 681/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0405 - acc: 0.9933 - val_loss: 0.0726 - val_acc: 0.9889\n",
            "Epoch 682/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0382 - acc: 0.9943 - val_loss: 0.0392 - val_acc: 0.9942\n",
            "Epoch 683/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0788 - acc: 0.9901 - val_loss: 0.0499 - val_acc: 0.9934\n",
            "Epoch 684/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0309 - acc: 0.9954 - val_loss: 0.1348 - val_acc: 0.9818\n",
            "Epoch 685/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0648 - acc: 0.9907 - val_loss: 0.4620 - val_acc: 0.9136\n",
            "Epoch 686/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0398 - acc: 0.9933 - val_loss: 0.0312 - val_acc: 0.9957\n",
            "Epoch 687/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0493 - acc: 0.9916 - val_loss: 0.0363 - val_acc: 0.9946\n",
            "Epoch 688/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0340 - acc: 0.9943 - val_loss: 0.1324 - val_acc: 0.9821\n",
            "Epoch 689/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0294 - acc: 0.9948 - val_loss: 0.0220 - val_acc: 0.9974\n",
            "Epoch 690/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0697 - acc: 0.9903 - val_loss: 0.3193 - val_acc: 0.9335\n",
            "Epoch 691/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0622 - acc: 0.9908 - val_loss: 0.0649 - val_acc: 0.9872\n",
            "Epoch 692/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0690 - acc: 0.9890 - val_loss: 0.0606 - val_acc: 0.9924\n",
            "Epoch 693/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0428 - acc: 0.9940 - val_loss: 0.0323 - val_acc: 0.9960\n",
            "Epoch 694/1000\n",
            "54924/54924 [==============================] - 2s 34us/step - loss: 0.0489 - acc: 0.9922 - val_loss: 0.0246 - val_acc: 0.9973\n",
            "Epoch 695/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0281 - acc: 0.9956 - val_loss: 0.0412 - val_acc: 0.9940\n",
            "Epoch 696/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0349 - acc: 0.9938 - val_loss: 0.0197 - val_acc: 0.9982\n",
            "Epoch 697/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0431 - acc: 0.9935 - val_loss: 0.0385 - val_acc: 0.9937\n",
            "Epoch 698/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1289 - acc: 0.9870 - val_loss: 0.0299 - val_acc: 0.9961\n",
            "Epoch 699/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0514 - acc: 0.9921 - val_loss: 0.0498 - val_acc: 0.9926\n",
            "Epoch 700/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0300 - acc: 0.9946 - val_loss: 0.0273 - val_acc: 0.9955\n",
            "Epoch 701/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0554 - acc: 0.9920 - val_loss: 0.0211 - val_acc: 0.9977\n",
            "Epoch 702/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0339 - acc: 0.9946 - val_loss: 0.0419 - val_acc: 0.9923\n",
            "Epoch 703/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0455 - acc: 0.9923 - val_loss: 0.0299 - val_acc: 0.9964\n",
            "Epoch 704/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0535 - acc: 0.9918 - val_loss: 0.0487 - val_acc: 0.9910\n",
            "Epoch 705/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0700 - acc: 0.9919 - val_loss: 0.0177 - val_acc: 0.9975\n",
            "Epoch 706/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0329 - acc: 0.9948 - val_loss: 0.0273 - val_acc: 0.9956\n",
            "Epoch 707/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0349 - acc: 0.9938 - val_loss: 0.0498 - val_acc: 0.9913\n",
            "Epoch 708/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0403 - acc: 0.9936 - val_loss: 0.0752 - val_acc: 0.9883\n",
            "Epoch 709/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0273 - acc: 0.9957 - val_loss: 0.0543 - val_acc: 0.9898\n",
            "Epoch 710/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0474 - acc: 0.9935 - val_loss: 0.0293 - val_acc: 0.9964\n",
            "Epoch 711/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0388 - acc: 0.9937 - val_loss: 0.0323 - val_acc: 0.9959\n",
            "Epoch 712/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0350 - acc: 0.9944 - val_loss: 0.0180 - val_acc: 0.9975\n",
            "Epoch 713/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0451 - acc: 0.9922 - val_loss: 0.0165 - val_acc: 0.9983\n",
            "Epoch 714/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0250 - acc: 0.9953 - val_loss: 0.0206 - val_acc: 0.9964\n",
            "Epoch 715/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0574 - acc: 0.9922 - val_loss: 0.0215 - val_acc: 0.9977\n",
            "Epoch 716/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0580 - acc: 0.9916 - val_loss: 0.0872 - val_acc: 0.9865\n",
            "Epoch 717/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0514 - acc: 0.9920 - val_loss: 0.0367 - val_acc: 0.9943\n",
            "Epoch 718/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0413 - acc: 0.9933 - val_loss: 0.0221 - val_acc: 0.9977\n",
            "Epoch 719/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0439 - acc: 0.9937 - val_loss: 0.0337 - val_acc: 0.9954\n",
            "Epoch 720/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0249 - acc: 0.9961 - val_loss: 0.0385 - val_acc: 0.9943\n",
            "Epoch 721/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.1697 - acc: 0.9836 - val_loss: 0.0280 - val_acc: 0.9970\n",
            "Epoch 722/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0591 - acc: 0.9916 - val_loss: 0.1528 - val_acc: 0.9808\n",
            "Epoch 723/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0548 - acc: 0.9918 - val_loss: 0.2300 - val_acc: 0.9768\n",
            "Epoch 724/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1185 - acc: 0.9864 - val_loss: 0.0299 - val_acc: 0.9969\n",
            "Epoch 725/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0445 - acc: 0.9929 - val_loss: 0.0266 - val_acc: 0.9970\n",
            "Epoch 726/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0238 - acc: 0.9958 - val_loss: 0.0706 - val_acc: 0.9883\n",
            "Epoch 727/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0276 - acc: 0.9955 - val_loss: 0.0402 - val_acc: 0.9923\n",
            "Epoch 728/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0737 - acc: 0.9893 - val_loss: 0.1873 - val_acc: 0.9595\n",
            "Epoch 729/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0468 - acc: 0.9930 - val_loss: 0.0382 - val_acc: 0.9931\n",
            "Epoch 730/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0359 - acc: 0.9943 - val_loss: 0.0267 - val_acc: 0.9968\n",
            "Epoch 731/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0371 - acc: 0.9944 - val_loss: 0.0290 - val_acc: 0.9964\n",
            "Epoch 732/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0824 - acc: 0.9903 - val_loss: 0.0425 - val_acc: 0.9948\n",
            "Epoch 733/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0314 - acc: 0.9950 - val_loss: 0.0323 - val_acc: 0.9961\n",
            "Epoch 734/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0476 - acc: 0.9923 - val_loss: 0.0227 - val_acc: 0.9972\n",
            "Epoch 735/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0510 - acc: 0.9923 - val_loss: 0.0911 - val_acc: 0.9833\n",
            "Epoch 736/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0444 - acc: 0.9934 - val_loss: 0.0179 - val_acc: 0.9976\n",
            "Epoch 737/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0426 - acc: 0.9932 - val_loss: 0.0147 - val_acc: 0.9981\n",
            "Epoch 738/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0353 - acc: 0.9940 - val_loss: 0.0167 - val_acc: 0.9977\n",
            "Epoch 739/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0328 - acc: 0.9947 - val_loss: 0.0205 - val_acc: 0.9975\n",
            "Epoch 740/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0547 - acc: 0.9928 - val_loss: 0.0394 - val_acc: 0.9921\n",
            "Epoch 741/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.2449 - acc: 0.9790 - val_loss: 0.0419 - val_acc: 0.9932\n",
            "Epoch 742/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0373 - acc: 0.9936 - val_loss: 0.0285 - val_acc: 0.9977\n",
            "Epoch 743/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0282 - acc: 0.9953 - val_loss: 0.0225 - val_acc: 0.9974\n",
            "Epoch 744/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0564 - acc: 0.9919 - val_loss: 0.0317 - val_acc: 0.9975\n",
            "Epoch 745/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0422 - acc: 0.9939 - val_loss: 0.0155 - val_acc: 0.9982\n",
            "Epoch 746/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0359 - acc: 0.9944 - val_loss: 0.1070 - val_acc: 0.9860\n",
            "Epoch 747/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0640 - acc: 0.9916 - val_loss: 0.0592 - val_acc: 0.9875\n",
            "Epoch 748/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0344 - acc: 0.9949 - val_loss: 0.0239 - val_acc: 0.9975\n",
            "Epoch 749/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0608 - acc: 0.9913 - val_loss: 0.0369 - val_acc: 0.9945\n",
            "Epoch 750/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0315 - acc: 0.9943 - val_loss: 0.0319 - val_acc: 0.9964\n",
            "Epoch 751/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0495 - acc: 0.9926 - val_loss: 0.0286 - val_acc: 0.9971\n",
            "Epoch 752/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0357 - acc: 0.9939 - val_loss: 0.0774 - val_acc: 0.9881\n",
            "Epoch 753/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0326 - acc: 0.9942 - val_loss: 0.0265 - val_acc: 0.9971\n",
            "Epoch 754/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0394 - acc: 0.9938 - val_loss: 0.0255 - val_acc: 0.9974\n",
            "Epoch 755/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0584 - acc: 0.9922 - val_loss: 0.0171 - val_acc: 0.9977\n",
            "Epoch 756/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0674 - acc: 0.9887 - val_loss: 0.0507 - val_acc: 0.9926\n",
            "Epoch 757/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0375 - acc: 0.9936 - val_loss: 0.0992 - val_acc: 0.9851\n",
            "Epoch 758/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0270 - acc: 0.9957 - val_loss: 0.0951 - val_acc: 0.9861\n",
            "Epoch 759/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0451 - acc: 0.9931 - val_loss: 0.0827 - val_acc: 0.9832\n",
            "Epoch 760/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0336 - acc: 0.9943 - val_loss: 0.0197 - val_acc: 0.9977\n",
            "Epoch 761/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0335 - acc: 0.9941 - val_loss: 0.0340 - val_acc: 0.9950\n",
            "Epoch 762/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0515 - acc: 0.9911 - val_loss: 0.0432 - val_acc: 0.9937\n",
            "Epoch 763/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0368 - acc: 0.9946 - val_loss: 0.0181 - val_acc: 0.9980\n",
            "Epoch 764/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0408 - acc: 0.9942 - val_loss: 0.0226 - val_acc: 0.9977\n",
            "Epoch 765/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0479 - acc: 0.9919 - val_loss: 0.0392 - val_acc: 0.9942\n",
            "Epoch 766/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.1366 - acc: 0.9858 - val_loss: 0.1199 - val_acc: 0.9839\n",
            "Epoch 767/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0490 - acc: 0.9910 - val_loss: 0.0494 - val_acc: 0.9925\n",
            "Epoch 768/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0304 - acc: 0.9947 - val_loss: 0.1026 - val_acc: 0.9846\n",
            "Epoch 769/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0662 - acc: 0.9893 - val_loss: 0.1278 - val_acc: 0.9833\n",
            "Epoch 770/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0744 - acc: 0.9891 - val_loss: 0.0330 - val_acc: 0.9955\n",
            "Epoch 771/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0499 - acc: 0.9917 - val_loss: 0.0287 - val_acc: 0.9958\n",
            "Epoch 772/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0474 - acc: 0.9925 - val_loss: 0.0445 - val_acc: 0.9936\n",
            "Epoch 773/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0988 - acc: 0.9889 - val_loss: 0.2384 - val_acc: 0.9755\n",
            "Epoch 774/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0616 - acc: 0.9917 - val_loss: 0.0458 - val_acc: 0.9937\n",
            "Epoch 775/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0571 - acc: 0.9904 - val_loss: 0.0463 - val_acc: 0.9932\n",
            "Epoch 776/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1223 - acc: 0.9864 - val_loss: 0.0706 - val_acc: 0.9897\n",
            "Epoch 777/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0696 - acc: 0.9899 - val_loss: 0.0269 - val_acc: 0.9958\n",
            "Epoch 778/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0417 - acc: 0.9936 - val_loss: 0.0213 - val_acc: 0.9963\n",
            "Epoch 779/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0439 - acc: 0.9932 - val_loss: 0.0297 - val_acc: 0.9963\n",
            "Epoch 780/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0532 - acc: 0.9918 - val_loss: 0.0252 - val_acc: 0.9972\n",
            "Epoch 781/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0391 - acc: 0.9946 - val_loss: 0.1819 - val_acc: 0.9532\n",
            "Epoch 782/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0421 - acc: 0.9930 - val_loss: 0.0196 - val_acc: 0.9970\n",
            "Epoch 783/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0717 - acc: 0.9907 - val_loss: 0.0323 - val_acc: 0.9960\n",
            "Epoch 784/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0376 - acc: 0.9941 - val_loss: 0.0223 - val_acc: 0.9974\n",
            "Epoch 785/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0283 - acc: 0.9963 - val_loss: 0.0305 - val_acc: 0.9965\n",
            "Epoch 786/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0441 - acc: 0.9935 - val_loss: 0.0315 - val_acc: 0.9959\n",
            "Epoch 787/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0484 - acc: 0.9915 - val_loss: 0.0192 - val_acc: 0.9971\n",
            "Epoch 788/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0337 - acc: 0.9945 - val_loss: 0.0400 - val_acc: 0.9942\n",
            "Epoch 789/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0469 - acc: 0.9940 - val_loss: 0.0779 - val_acc: 0.9886\n",
            "Epoch 790/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0346 - acc: 0.9942 - val_loss: 0.0366 - val_acc: 0.9949\n",
            "Epoch 791/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0504 - acc: 0.9933 - val_loss: 0.0240 - val_acc: 0.9967\n",
            "Epoch 792/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0474 - acc: 0.9920 - val_loss: 0.0837 - val_acc: 0.9881\n",
            "Epoch 793/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0552 - acc: 0.9916 - val_loss: 0.0299 - val_acc: 0.9958\n",
            "Epoch 794/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0399 - acc: 0.9933 - val_loss: 0.0113 - val_acc: 0.9986\n",
            "Epoch 795/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0431 - acc: 0.9929 - val_loss: 0.1406 - val_acc: 0.9822\n",
            "Epoch 796/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0371 - acc: 0.9941 - val_loss: 0.0131 - val_acc: 0.9980\n",
            "Epoch 797/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0453 - acc: 0.9930 - val_loss: 0.0220 - val_acc: 0.9979\n",
            "Epoch 798/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0270 - acc: 0.9954 - val_loss: 0.0402 - val_acc: 0.9941\n",
            "Epoch 799/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1620 - acc: 0.9845 - val_loss: 0.0349 - val_acc: 0.9958\n",
            "Epoch 800/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0302 - acc: 0.9949 - val_loss: 0.0420 - val_acc: 0.9943\n",
            "Epoch 801/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0362 - acc: 0.9944 - val_loss: 0.0180 - val_acc: 0.9975\n",
            "Epoch 802/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0470 - acc: 0.9936 - val_loss: 0.1495 - val_acc: 0.9808\n",
            "Epoch 803/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0785 - acc: 0.9900 - val_loss: 0.0481 - val_acc: 0.9926\n",
            "Epoch 804/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0491 - acc: 0.9913 - val_loss: 0.0496 - val_acc: 0.9926\n",
            "Epoch 805/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0573 - acc: 0.9925 - val_loss: 0.0329 - val_acc: 0.9958\n",
            "Epoch 806/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0642 - acc: 0.9906 - val_loss: 0.0257 - val_acc: 0.9972\n",
            "Epoch 807/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0239 - acc: 0.9960 - val_loss: 0.0590 - val_acc: 0.9913\n",
            "Epoch 808/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0648 - acc: 0.9921 - val_loss: 0.0262 - val_acc: 0.9977\n",
            "Epoch 809/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0497 - acc: 0.9942 - val_loss: 0.0171 - val_acc: 0.9983\n",
            "Epoch 810/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0668 - acc: 0.9904 - val_loss: 0.0188 - val_acc: 0.9975\n",
            "Epoch 811/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0444 - acc: 0.9926 - val_loss: 0.0307 - val_acc: 0.9961\n",
            "Epoch 812/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0300 - acc: 0.9953 - val_loss: 0.0096 - val_acc: 0.9987\n",
            "Epoch 813/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0456 - acc: 0.9932 - val_loss: 0.0375 - val_acc: 0.9944\n",
            "Epoch 814/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0428 - acc: 0.9935 - val_loss: 0.0140 - val_acc: 0.9984\n",
            "Epoch 815/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0501 - acc: 0.9927 - val_loss: 0.1871 - val_acc: 0.9782\n",
            "Epoch 816/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0569 - acc: 0.9928 - val_loss: 0.0227 - val_acc: 0.9970\n",
            "Epoch 817/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0566 - acc: 0.9913 - val_loss: 0.0151 - val_acc: 0.9986\n",
            "Epoch 818/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0688 - acc: 0.9905 - val_loss: 0.1021 - val_acc: 0.9861\n",
            "Epoch 819/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0320 - acc: 0.9946 - val_loss: 0.0521 - val_acc: 0.9916\n",
            "Epoch 820/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0367 - acc: 0.9941 - val_loss: 0.0237 - val_acc: 0.9975\n",
            "Epoch 821/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0384 - acc: 0.9939 - val_loss: 0.0230 - val_acc: 0.9971\n",
            "Epoch 822/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0479 - acc: 0.9924 - val_loss: 0.0620 - val_acc: 0.9912\n",
            "Epoch 823/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0358 - acc: 0.9950 - val_loss: 0.0404 - val_acc: 0.9950\n",
            "Epoch 824/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0245 - acc: 0.9961 - val_loss: 0.0176 - val_acc: 0.9976\n",
            "Epoch 825/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0328 - acc: 0.9949 - val_loss: 0.0299 - val_acc: 0.9961\n",
            "Epoch 826/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0325 - acc: 0.9953 - val_loss: 0.0325 - val_acc: 0.9956\n",
            "Epoch 827/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0287 - acc: 0.9956 - val_loss: 0.0450 - val_acc: 0.9931\n",
            "Epoch 828/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0452 - acc: 0.9934 - val_loss: 0.0484 - val_acc: 0.9914\n",
            "Epoch 829/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0530 - acc: 0.9917 - val_loss: 0.2277 - val_acc: 0.9767\n",
            "Epoch 830/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0461 - acc: 0.9927 - val_loss: 0.0122 - val_acc: 0.9984\n",
            "Epoch 831/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0559 - acc: 0.9931 - val_loss: 0.0304 - val_acc: 0.9961\n",
            "Epoch 832/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0394 - acc: 0.9950 - val_loss: 0.0384 - val_acc: 0.9947\n",
            "Epoch 833/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0318 - acc: 0.9951 - val_loss: 0.0125 - val_acc: 0.9979\n",
            "Epoch 834/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0469 - acc: 0.9927 - val_loss: 0.0181 - val_acc: 0.9978\n",
            "Epoch 835/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0395 - acc: 0.9942 - val_loss: 0.0230 - val_acc: 0.9973\n",
            "Epoch 836/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0334 - acc: 0.9952 - val_loss: 0.0277 - val_acc: 0.9967\n",
            "Epoch 837/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.1241 - acc: 0.9874 - val_loss: 0.0628 - val_acc: 0.9917\n",
            "Epoch 838/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0488 - acc: 0.9931 - val_loss: 0.0342 - val_acc: 0.9956\n",
            "Epoch 839/1000\n",
            "54924/54924 [==============================] - 2s 34us/step - loss: 0.0467 - acc: 0.9933 - val_loss: 0.0263 - val_acc: 0.9956\n",
            "Epoch 840/1000\n",
            "54924/54924 [==============================] - 2s 32us/step - loss: 0.0462 - acc: 0.9940 - val_loss: 0.2768 - val_acc: 0.9449\n",
            "Epoch 841/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0589 - acc: 0.9911 - val_loss: 0.0315 - val_acc: 0.9964\n",
            "Epoch 842/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0370 - acc: 0.9944 - val_loss: 0.0151 - val_acc: 0.9980\n",
            "Epoch 843/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0466 - acc: 0.9940 - val_loss: 0.0244 - val_acc: 0.9969\n",
            "Epoch 844/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0324 - acc: 0.9952 - val_loss: 0.0303 - val_acc: 0.9964\n",
            "Epoch 845/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0436 - acc: 0.9940 - val_loss: 0.0335 - val_acc: 0.9958\n",
            "Epoch 846/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0310 - acc: 0.9951 - val_loss: 0.0131 - val_acc: 0.9987\n",
            "Epoch 847/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0509 - acc: 0.9922 - val_loss: 0.0159 - val_acc: 0.9983\n",
            "Epoch 848/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0655 - acc: 0.9911 - val_loss: 0.0536 - val_acc: 0.9920\n",
            "Epoch 849/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0211 - acc: 0.9970 - val_loss: 0.0347 - val_acc: 0.9950\n",
            "Epoch 850/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0362 - acc: 0.9947 - val_loss: 0.0383 - val_acc: 0.9950\n",
            "Epoch 851/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0414 - acc: 0.9934 - val_loss: 0.0537 - val_acc: 0.9922\n",
            "Epoch 852/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0490 - acc: 0.9929 - val_loss: 0.0255 - val_acc: 0.9969\n",
            "Epoch 853/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0343 - acc: 0.9950 - val_loss: 0.0321 - val_acc: 0.9958\n",
            "Epoch 854/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0399 - acc: 0.9940 - val_loss: 0.0207 - val_acc: 0.9973\n",
            "Epoch 855/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0562 - acc: 0.9917 - val_loss: 0.0246 - val_acc: 0.9977\n",
            "Epoch 856/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0382 - acc: 0.9938 - val_loss: 0.0541 - val_acc: 0.9918\n",
            "Epoch 857/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0330 - acc: 0.9950 - val_loss: 0.0196 - val_acc: 0.9976\n",
            "Epoch 858/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0833 - acc: 0.9887 - val_loss: 0.0374 - val_acc: 0.9952\n",
            "Epoch 859/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0446 - acc: 0.9948 - val_loss: 0.1051 - val_acc: 0.9859\n",
            "Epoch 860/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0309 - acc: 0.9957 - val_loss: 0.0103 - val_acc: 0.9986\n",
            "Epoch 861/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0346 - acc: 0.9943 - val_loss: 0.0121 - val_acc: 0.9985\n",
            "Epoch 862/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0628 - acc: 0.9906 - val_loss: 0.0162 - val_acc: 0.9980\n",
            "Epoch 863/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0410 - acc: 0.9941 - val_loss: 0.0995 - val_acc: 0.9857\n",
            "Epoch 864/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1144 - acc: 0.9880 - val_loss: 0.0470 - val_acc: 0.9942\n",
            "Epoch 865/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0337 - acc: 0.9949 - val_loss: 0.1004 - val_acc: 0.9857\n",
            "Epoch 866/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0374 - acc: 0.9946 - val_loss: 0.0146 - val_acc: 0.9981\n",
            "Epoch 867/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0284 - acc: 0.9955 - val_loss: 0.0198 - val_acc: 0.9975\n",
            "Epoch 868/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0572 - acc: 0.9916 - val_loss: 0.0333 - val_acc: 0.9953\n",
            "Epoch 869/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0277 - acc: 0.9953 - val_loss: 0.0562 - val_acc: 0.9916\n",
            "Epoch 870/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0451 - acc: 0.9932 - val_loss: 0.0512 - val_acc: 0.9933\n",
            "Epoch 871/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0608 - acc: 0.9917 - val_loss: 0.0284 - val_acc: 0.9971\n",
            "Epoch 872/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0269 - acc: 0.9960 - val_loss: 0.0137 - val_acc: 0.9985\n",
            "Epoch 873/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0583 - acc: 0.9910 - val_loss: 0.0143 - val_acc: 0.9980\n",
            "Epoch 874/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0858 - acc: 0.9885 - val_loss: 0.0267 - val_acc: 0.9958\n",
            "Epoch 875/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0297 - acc: 0.9958 - val_loss: 0.0676 - val_acc: 0.9892\n",
            "Epoch 876/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0328 - acc: 0.9950 - val_loss: 0.0300 - val_acc: 0.9964\n",
            "Epoch 877/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0662 - acc: 0.9908 - val_loss: 0.0238 - val_acc: 0.9969\n",
            "Epoch 878/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.0396 - acc: 0.9943 - val_loss: 0.0307 - val_acc: 0.9965\n",
            "Epoch 879/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0358 - acc: 0.9949 - val_loss: 0.0346 - val_acc: 0.9952\n",
            "Epoch 880/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0320 - acc: 0.9954 - val_loss: 0.0195 - val_acc: 0.9980\n",
            "Epoch 881/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0373 - acc: 0.9945 - val_loss: 0.0154 - val_acc: 0.9983\n",
            "Epoch 882/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0410 - acc: 0.9942 - val_loss: 0.0095 - val_acc: 0.9985\n",
            "Epoch 883/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0393 - acc: 0.9941 - val_loss: 0.1942 - val_acc: 0.9783\n",
            "Epoch 884/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0335 - acc: 0.9950 - val_loss: 0.2425 - val_acc: 0.9772\n",
            "Epoch 885/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0418 - acc: 0.9945 - val_loss: 0.0221 - val_acc: 0.9972\n",
            "Epoch 886/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0588 - acc: 0.9917 - val_loss: 0.0727 - val_acc: 0.9890\n",
            "Epoch 887/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0899 - acc: 0.9880 - val_loss: 0.1778 - val_acc: 0.9797\n",
            "Epoch 888/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0472 - acc: 0.9936 - val_loss: 0.0141 - val_acc: 0.9982\n",
            "Epoch 889/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0524 - acc: 0.9915 - val_loss: 0.0136 - val_acc: 0.9980\n",
            "Epoch 890/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0366 - acc: 0.9951 - val_loss: 0.0418 - val_acc: 0.9939\n",
            "Epoch 891/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0613 - acc: 0.9918 - val_loss: 0.0278 - val_acc: 0.9968\n",
            "Epoch 892/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0318 - acc: 0.9953 - val_loss: 0.0108 - val_acc: 0.9985\n",
            "Epoch 893/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0215 - acc: 0.9971 - val_loss: 0.0136 - val_acc: 0.9984\n",
            "Epoch 894/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.0406 - acc: 0.9950 - val_loss: 0.0916 - val_acc: 0.9873\n",
            "Epoch 895/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0563 - acc: 0.9913 - val_loss: 0.0094 - val_acc: 0.9988\n",
            "Epoch 896/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0381 - acc: 0.9946 - val_loss: 0.0240 - val_acc: 0.9971\n",
            "Epoch 897/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0579 - acc: 0.9917 - val_loss: 0.0187 - val_acc: 0.9976\n",
            "Epoch 898/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0408 - acc: 0.9937 - val_loss: 0.0582 - val_acc: 0.9921\n",
            "Epoch 899/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.0488 - acc: 0.9924 - val_loss: 0.0377 - val_acc: 0.9941\n",
            "Epoch 900/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0432 - acc: 0.9936 - val_loss: 0.0408 - val_acc: 0.9953\n",
            "Epoch 901/1000\n",
            "54924/54924 [==============================] - 1s 20us/step - loss: 0.0310 - acc: 0.9949 - val_loss: 0.3537 - val_acc: 0.9248\n",
            "Epoch 902/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.0641 - acc: 0.9920 - val_loss: 0.0230 - val_acc: 0.9978\n",
            "Epoch 903/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0361 - acc: 0.9955 - val_loss: 0.0177 - val_acc: 0.9982\n",
            "Epoch 904/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0564 - acc: 0.9923 - val_loss: 0.0388 - val_acc: 0.9947\n",
            "Epoch 905/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0381 - acc: 0.9944 - val_loss: 0.0349 - val_acc: 0.9930\n",
            "Epoch 906/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0313 - acc: 0.9950 - val_loss: 0.0144 - val_acc: 0.9983\n",
            "Epoch 907/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0515 - acc: 0.9922 - val_loss: 0.0307 - val_acc: 0.9964\n",
            "Epoch 908/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0453 - acc: 0.9940 - val_loss: 0.0365 - val_acc: 0.9950\n",
            "Epoch 909/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0436 - acc: 0.9926 - val_loss: 0.0099 - val_acc: 0.9990\n",
            "Epoch 910/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0470 - acc: 0.9932 - val_loss: 0.0290 - val_acc: 0.9967\n",
            "Epoch 911/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0435 - acc: 0.9934 - val_loss: 0.0488 - val_acc: 0.9928\n",
            "Epoch 912/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.0429 - acc: 0.9938 - val_loss: 0.0337 - val_acc: 0.9957\n",
            "Epoch 913/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0764 - acc: 0.9898 - val_loss: 0.0449 - val_acc: 0.9940\n",
            "Epoch 914/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0261 - acc: 0.9961 - val_loss: 0.0080 - val_acc: 0.9991\n",
            "Epoch 915/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0282 - acc: 0.9957 - val_loss: 0.0194 - val_acc: 0.9979\n",
            "Epoch 916/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.0288 - acc: 0.9952 - val_loss: 0.0417 - val_acc: 0.9942\n",
            "Epoch 917/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0561 - acc: 0.9911 - val_loss: 0.0507 - val_acc: 0.9893\n",
            "Epoch 918/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0312 - acc: 0.9954 - val_loss: 0.0102 - val_acc: 0.9985\n",
            "Epoch 919/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0205 - acc: 0.9967 - val_loss: 0.0080 - val_acc: 0.9990\n",
            "Epoch 920/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0511 - acc: 0.9924 - val_loss: 0.1593 - val_acc: 0.9811\n",
            "Epoch 921/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0415 - acc: 0.9946 - val_loss: 0.0115 - val_acc: 0.9985\n",
            "Epoch 922/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0444 - acc: 0.9944 - val_loss: 0.0156 - val_acc: 0.9983\n",
            "Epoch 923/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0404 - acc: 0.9943 - val_loss: 0.0096 - val_acc: 0.9991\n",
            "Epoch 924/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0392 - acc: 0.9931 - val_loss: 0.0830 - val_acc: 0.9879\n",
            "Epoch 925/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0705 - acc: 0.9912 - val_loss: 0.0205 - val_acc: 0.9979\n",
            "Epoch 926/1000\n",
            "54924/54924 [==============================] - 1s 20us/step - loss: 0.0246 - acc: 0.9964 - val_loss: 0.0113 - val_acc: 0.9983\n",
            "Epoch 927/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.0422 - acc: 0.9922 - val_loss: 0.1628 - val_acc: 0.9804\n",
            "Epoch 928/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0528 - acc: 0.9923 - val_loss: 0.0356 - val_acc: 0.9953\n",
            "Epoch 929/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0330 - acc: 0.9945 - val_loss: 0.0239 - val_acc: 0.9970\n",
            "Epoch 930/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0301 - acc: 0.9951 - val_loss: 0.0159 - val_acc: 0.9980\n",
            "Epoch 931/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0290 - acc: 0.9955 - val_loss: 0.0130 - val_acc: 0.9983\n",
            "Epoch 932/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0291 - acc: 0.9960 - val_loss: 0.0419 - val_acc: 0.9945\n",
            "Epoch 933/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.0395 - acc: 0.9947 - val_loss: 0.0331 - val_acc: 0.9956\n",
            "Epoch 934/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.0414 - acc: 0.9942 - val_loss: 0.0571 - val_acc: 0.9913\n",
            "Epoch 935/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0336 - acc: 0.9947 - val_loss: 0.0999 - val_acc: 0.9866\n",
            "Epoch 936/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.0292 - acc: 0.9957 - val_loss: 0.0251 - val_acc: 0.9946\n",
            "Epoch 937/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.0463 - acc: 0.9933 - val_loss: 0.0556 - val_acc: 0.9886\n",
            "Epoch 938/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0370 - acc: 0.9948 - val_loss: 0.0288 - val_acc: 0.9969\n",
            "Epoch 939/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0228 - acc: 0.9967 - val_loss: 0.0106 - val_acc: 0.9985\n",
            "Epoch 940/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0305 - acc: 0.9953 - val_loss: 0.0089 - val_acc: 0.9987\n",
            "Epoch 941/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0316 - acc: 0.9950 - val_loss: 0.0723 - val_acc: 0.9846\n",
            "Epoch 942/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.0327 - acc: 0.9945 - val_loss: 0.0220 - val_acc: 0.9961\n",
            "Epoch 943/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0427 - acc: 0.9939 - val_loss: 0.0120 - val_acc: 0.9985\n",
            "Epoch 944/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0352 - acc: 0.9945 - val_loss: 0.0280 - val_acc: 0.9969\n",
            "Epoch 945/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0450 - acc: 0.9932 - val_loss: 0.0263 - val_acc: 0.9969\n",
            "Epoch 946/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0315 - acc: 0.9950 - val_loss: 0.0327 - val_acc: 0.9954\n",
            "Epoch 947/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.0274 - acc: 0.9959 - val_loss: 0.0202 - val_acc: 0.9979\n",
            "Epoch 948/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.0205 - acc: 0.9969 - val_loss: 0.0253 - val_acc: 0.9970\n",
            "Epoch 949/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0239 - acc: 0.9959 - val_loss: 0.0682 - val_acc: 0.9897\n",
            "Epoch 950/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0299 - acc: 0.9954 - val_loss: 0.0118 - val_acc: 0.9985\n",
            "Epoch 951/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.0291 - acc: 0.9951 - val_loss: 0.0128 - val_acc: 0.9982\n",
            "Epoch 952/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0348 - acc: 0.9942 - val_loss: 0.0303 - val_acc: 0.9964\n",
            "Epoch 953/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0553 - acc: 0.9917 - val_loss: 0.1111 - val_acc: 0.9669\n",
            "Epoch 954/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0345 - acc: 0.9949 - val_loss: 0.0137 - val_acc: 0.9985\n",
            "Epoch 955/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.0242 - acc: 0.9961 - val_loss: 0.0687 - val_acc: 0.9833\n",
            "Epoch 956/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0368 - acc: 0.9933 - val_loss: 0.0334 - val_acc: 0.9951\n",
            "Epoch 957/1000\n",
            "54924/54924 [==============================] - 1s 20us/step - loss: 0.0485 - acc: 0.9922 - val_loss: 0.0126 - val_acc: 0.9983\n",
            "Epoch 958/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0236 - acc: 0.9966 - val_loss: 0.0259 - val_acc: 0.9969\n",
            "Epoch 959/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.0284 - acc: 0.9958 - val_loss: 0.0325 - val_acc: 0.9955\n",
            "Epoch 960/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0400 - acc: 0.9935 - val_loss: 0.1044 - val_acc: 0.9873\n",
            "Epoch 961/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0386 - acc: 0.9945 - val_loss: 0.2905 - val_acc: 0.9743\n",
            "Epoch 962/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0359 - acc: 0.9946 - val_loss: 0.0593 - val_acc: 0.9907\n",
            "Epoch 963/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0327 - acc: 0.9953 - val_loss: 0.0198 - val_acc: 0.9978\n",
            "Epoch 964/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0431 - acc: 0.9933 - val_loss: 0.0338 - val_acc: 0.9953\n",
            "Epoch 965/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0302 - acc: 0.9950 - val_loss: 0.0225 - val_acc: 0.9976\n",
            "Epoch 966/1000\n",
            "54924/54924 [==============================] - 1s 20us/step - loss: 0.0356 - acc: 0.9948 - val_loss: 0.0242 - val_acc: 0.9976\n",
            "Epoch 967/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0443 - acc: 0.9939 - val_loss: 0.0272 - val_acc: 0.9967\n",
            "Epoch 968/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.0356 - acc: 0.9949 - val_loss: 0.1283 - val_acc: 0.9843\n",
            "Epoch 969/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0331 - acc: 0.9950 - val_loss: 0.1039 - val_acc: 0.9868\n",
            "Epoch 970/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0316 - acc: 0.9950 - val_loss: 0.0147 - val_acc: 0.9980\n",
            "Epoch 971/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0189 - acc: 0.9970 - val_loss: 0.0446 - val_acc: 0.9937\n",
            "Epoch 972/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0588 - acc: 0.9915 - val_loss: 0.0132 - val_acc: 0.9990\n",
            "Epoch 973/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0505 - acc: 0.9930 - val_loss: 0.0099 - val_acc: 0.9983\n",
            "Epoch 974/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0400 - acc: 0.9943 - val_loss: 0.0155 - val_acc: 0.9980\n",
            "Epoch 975/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0487 - acc: 0.9930 - val_loss: 0.0170 - val_acc: 0.9983\n",
            "Epoch 976/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0241 - acc: 0.9963 - val_loss: 0.0497 - val_acc: 0.9886\n",
            "Epoch 977/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0478 - acc: 0.9941 - val_loss: 0.1478 - val_acc: 0.9837\n",
            "Epoch 978/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.0481 - acc: 0.9948 - val_loss: 0.4569 - val_acc: 0.9662\n",
            "Epoch 979/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0803 - acc: 0.9912 - val_loss: 0.1073 - val_acc: 0.9858\n",
            "Epoch 980/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0509 - acc: 0.9929 - val_loss: 0.0365 - val_acc: 0.9948\n",
            "Epoch 981/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.0251 - acc: 0.9962 - val_loss: 0.0797 - val_acc: 0.9887\n",
            "Epoch 982/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0196 - acc: 0.9970 - val_loss: 0.0258 - val_acc: 0.9967\n",
            "Epoch 983/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.0271 - acc: 0.9958 - val_loss: 0.0243 - val_acc: 0.9972\n",
            "Epoch 984/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0284 - acc: 0.9952 - val_loss: 0.0263 - val_acc: 0.9970\n",
            "Epoch 985/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0255 - acc: 0.9959 - val_loss: 0.0299 - val_acc: 0.9964\n",
            "Epoch 986/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.0277 - acc: 0.9957 - val_loss: 0.0182 - val_acc: 0.9980\n",
            "Epoch 987/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.0307 - acc: 0.9942 - val_loss: 0.0139 - val_acc: 0.9983\n",
            "Epoch 988/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0291 - acc: 0.9951 - val_loss: 0.0264 - val_acc: 0.9969\n",
            "Epoch 989/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0432 - acc: 0.9939 - val_loss: 0.0117 - val_acc: 0.9984\n",
            "Epoch 990/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0861 - acc: 0.9910 - val_loss: 0.2449 - val_acc: 0.9767\n",
            "Epoch 991/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0444 - acc: 0.9941 - val_loss: 0.2059 - val_acc: 0.9619\n",
            "Epoch 992/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0291 - acc: 0.9960 - val_loss: 0.0279 - val_acc: 0.9937\n",
            "Epoch 993/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.0245 - acc: 0.9966 - val_loss: 0.0125 - val_acc: 0.9984\n",
            "Epoch 994/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0267 - acc: 0.9954 - val_loss: 0.0190 - val_acc: 0.9977\n",
            "Epoch 995/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0511 - acc: 0.9917 - val_loss: 0.0246 - val_acc: 0.9977\n",
            "Epoch 996/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0281 - acc: 0.9958 - val_loss: 0.0207 - val_acc: 0.9976\n",
            "Epoch 997/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0400 - acc: 0.9938 - val_loss: 0.0089 - val_acc: 0.9991\n",
            "Epoch 998/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.0326 - acc: 0.9944 - val_loss: 0.0086 - val_acc: 0.9986\n",
            "Epoch 999/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.0363 - acc: 0.9949 - val_loss: 0.0212 - val_acc: 0.9980\n",
            "Epoch 1000/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.0320 - acc: 0.9959 - val_loss: 0.0403 - val_acc: 0.9942\n",
            "[[27889   116]\n",
            " [    6  1413]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       1.00      1.00      1.00     28005\n",
            "        True       0.92      1.00      0.96      1419\n",
            "\n",
            "    accuracy                           1.00     29424\n",
            "   macro avg       0.96      1.00      0.98     29424\n",
            "weighted avg       1.00      1.00      1.00     29424\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noii+qLBQFEVRfFFhoSiKovjy/wEruPQJ08RQcQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUKWgKIqiJFCloCiKoiRQpaAoiqIk+P9W+31gNFYC7QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ww7P__tv12_",
        "colab_type": "code",
        "outputId": "e2f0d495-81af-4235-8962-d981c3a69f3d",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "#Test a set of data not seen before, and with no classification to emulate user input\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "import pandas as pd  \n",
        "userdata = pd.read_csv(\"TestUserDataTrue.csv\")\n",
        "\n",
        "userdata_X = userdata.values\n",
        "userdata_y = userdata.Defective.values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f8b965ef-cc9f-421b-925d-df862a90f0f5\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-f8b965ef-cc9f-421b-925d-df862a90f0f5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving TestUserDataTrue.csv to TestUserDataTrue.csv\n",
            "User uploaded file \"TestUserDataTrue.csv\" with length 386 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTHWxxyEv2vw",
        "colab_type": "code",
        "outputId": "021dbdd9-f594-42f3-e6f2-815711ba0939",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "user_predict = model.predict(userdata_X)\n",
        "user_predict =(user_predict>0.5)\n",
        "print(confusion_matrix(userdata_y, user_predict))\n",
        "print(classification_report(userdata_y, user_predict))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5b5529fe65c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0muser_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muserdata_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0muser_predict\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_predict\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muserdata_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muserdata_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLob6W5-zsvk",
        "colab_type": "text"
      },
      "source": [
        "#**Linear** Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guRepAVacruW",
        "colab_type": "code",
        "outputId": "fbeb66ce-7a17-4cf2-b833-695e01358c56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#Selecting the Csv to use then Displaying it, may have to change the (3) when first ran\n",
        "df = pd.read_csv(\"FullELFFDataset.csv\")\n",
        "df[0:10]\n",
        "\n",
        "#Seperating the data into two sections, training and testing\n",
        "X = df.drop(['Defective'], axis='columns')\n",
        "y = df.Defective\n",
        "#Split with a weighing of 70-30(0.3)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n",
        "\n",
        "#Running the Regression Model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "clf = LinearRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "#Run our Prediction of what the X_test should look like\n",
        "clf.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix \n",
        "\n",
        "#Our accuracy percentage\n",
        "#clf.score(X_test, y_test)\n",
        "\n",
        "Y_predict = clf.predict(X_test)\n",
        "Y_predict =(Y_predict>0.5)\n",
        "print(confusion_matrix(y_test, Y_predict))\n",
        "print(classification_report(y_test, Y_predict))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[27947     6]\n",
            " [ 1300   171]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.96      1.00      0.98     27953\n",
            "        True       0.97      0.12      0.21      1471\n",
            "\n",
            "    accuracy                           0.96     29424\n",
            "   macro avg       0.96      0.56      0.59     29424\n",
            "weighted avg       0.96      0.96      0.94     29424\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAnjFbRmwcg7",
        "colab_type": "text"
      },
      "source": [
        "# **Support Vector Machine**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S2n5N0qwt8I",
        "colab_type": "code",
        "outputId": "4d3012de-0b68-4508-ce3e-9efc54f45af6",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "#Uploading Test Data\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5cbbb103-0c1c-4c5d-8c9d-04c657275850\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-5cbbb103-0c1c-4c5d-8c9d-04c657275850\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving BalancedELFF.csv to BalancedELFF.csv\n",
            "User uploaded file \"BalancedELFF.csv\" with length 616292 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3z18xClwe6N",
        "colab_type": "code",
        "outputId": "16a22079-9c5f-4b95-9086-43033f7e17ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "#Imports\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Selecting the Csv to use then Displaying it, may have to change the (3) when first ran.\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"BalancedELFF.csv\")\n",
        "\n",
        "#Setting up the dataframes so that ones with defects get placed in dataframe1 and ones without get placed in dataframe0.\n",
        "df0 = df[df.Defective==False]\n",
        "df1 = df[df.Defective==True]\n",
        "\n",
        "#Dropping the classification so it can be predicted\n",
        "X = df.drop(['Defective'], axis='columns')\n",
        "Y = df.Defective\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2)\n",
        "trueWeight = len(Y_train[Y_train==False])\n",
        "falseWeight = len(Y_train[Y_train==True])\n",
        "\n",
        "print(\"Y_train 'False' Length:\", len(Y_train[Y_train==False]))\n",
        "print(\"Y_train 'True' Length:\", len(Y_train[Y_train==True]))\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix \n",
        "from sklearn import preprocessing\n",
        "X = preprocessing.scale(X) \n",
        "\n",
        "model = SVC(kernel='linear', verbose=10)\n",
        "\n",
        "#Training the Model using the fit method - Change these parameters to tune the model\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "Y_predict = model.predict(X_test)\n",
        "print(confusion_matrix(Y_test, Y_predict))\n",
        "print(classification_report(Y_test, Y_predict))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Y_train 'False' Length: 2151\n",
            "Y_train 'True' Length: 1065\n",
            "[LibSVM][[428 106]\n",
            " [ 77 193]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.85      0.80      0.82       534\n",
            "        True       0.65      0.71      0.68       270\n",
            "\n",
            "    accuracy                           0.77       804\n",
            "   macro avg       0.75      0.76      0.75       804\n",
            "weighted avg       0.78      0.77      0.78       804\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnjnKfVgxAKU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_predict = model.predict(X_test)\n",
        "cfm = confusion_matrix(Y_test, Y_predict)\n",
        "\n",
        "print(cfm)\n",
        "trueNeg = cfm[0,0]\n",
        "falseNeg = cfm[1,0]\n",
        "truePos = cfm[1,1]\n",
        "falsePos = cfm[0,1]\n",
        "\n",
        "#True Positive Rate\n",
        "truePosRate = truePos/(truePos+falseNeg)\n",
        "print(\"TruePos\", truePosRate)\n",
        "\n",
        "#True Negative Rate\n",
        "trueNegRate = trueNeg/(trueNeg+falsePos)\n",
        "print(\"TrueNeg\", trueNegRate)\n",
        "\n",
        "#Positive Prediciton Value - Pos Precision\n",
        "posPreVal = truePos/(truePos+falsePos)\n",
        "print(\"PosPrecision\", posPreVal)\n",
        "\n",
        "#Negative Prediciton Value - Neg Precision\n",
        "negPreVal = trueNeg/(trueNeg+falseNeg)\n",
        "print(\"NegPrecision\", negPreVal)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}